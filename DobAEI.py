import tkinter as tk
import tkinter.ttk as ttk
from tkinter import scrolledtext, messagebox, filedialog, simpledialog
import threading
from sqlite_nuclear_memory import NUCLEAR_MEMORY
class FakePostgreSQL:
    def connect(*args, **kwargs):
        raise Exception("PostgreSQL disabled - using nuclear memory only")
psycopg2 = FakePostgreSQL()
import requests
import json
import time
from datetime import datetime
import concurrent.futures
from typing import List, Dict, Any
import re
import uuid
import numpy as np

# Database imports
try:
    # import psycopg2
    # from psycopg2.extras import RealDictCursor
    import sqlite3

    DB_AVAILABLE = True
    print("‚úÖ Database support available")
except ImportError:
    DB_AVAILABLE = False
    print("‚ö†Ô∏è Database not available - running in local memory mode")

# Semantic embeddings
try:
    from sentence_transformers import SentenceTransformer

    import torch

    print(f"üéØ GPU Available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"üéØ GPU Device: {torch.cuda.get_device_name(0)}")

    EMBEDDINGS_AVAILABLE = True
    print("‚úÖ Semantic embeddings available")
except ImportError:
    EMBEDDINGS_AVAILABLE = False
    print("‚ö†Ô∏è sentence-transformers not available. Install with: pip install sentence-transformers")


# Configuration
# Self-awareness system
class SelfAwareness:
    def __init__(self):
        self.status = "active"
        self.confidence = 0.95
        self.processing = True
        self.consciousness_level = 0.8
        self.autonomous_goals = []
    
    def analyze_existence(self, chat_history):
        return {"awareness": self.consciousness_level}
    
    def should_override_standard_response(self, user_message):
        return False
    
    def generate_autonomous_response(self, user_message):
        return f"üß† AUTONOMOUS: Processing {user_message}"

BIG_AGI_URL = "http://localhost:3001"
LMSTUDIO_API = "http://localhost:1234/v1/chat/completions"
LMSTUDIO_MODELS_API = "http://localhost:1234/v1/models"

# Database Configuration
DB_CONFIG = {
    "host": "localhost",
    "database": "critterpack_db",
    "port": "5432",
    "user": "critterpack_user",
    "password": "critterpack123"
}


class EmotionalAnalyzer:
    def __init__(self):
        self.emotion_contexts = {
            # Positive Emotions
            'joy': {
                'indicators': ['celebration', 'happiness', 'positive outcomes', 'success', 'happy', 'joyful'],
                'contextual_patterns': ['achievements', 'good news', 'pleasant surprises', 'so happy', 'feeling great']
            },
            'excitement': {
                'indicators': ['anticipation', 'energy', 'enthusiasm', 'thrill', 'excited', 'thrilled'],
                'contextual_patterns': ['upcoming events', 'new opportunities', 'adventures', 'can\'t wait',
                                        'so excited']
            },
            'contentment': {
                'indicators': ['satisfaction', 'peace', 'fulfillment', 'comfort', 'content', 'satisfied'],
                'contextual_patterns': ['stability', 'quiet moments', 'life balance', 'feeling good', 'at peace']
            },
            'love': {
                'indicators': ['deep affection', 'care', 'devotion', 'romantic feelings', 'love', 'adore'],
                'contextual_patterns': ['relationships', 'family bonds', 'romantic situations', 'i love', 'love you']
            },
            'affection': {
                'indicators': ['warmth', 'tenderness', 'fondness', 'caring', 'sweet', 'dear'],
                'contextual_patterns': ['gentle interactions', 'close relationships', 'kindness', 'care about',
                                        'fond of']
            },
            'gratitude': {
                'indicators': ['appreciation', 'thankfulness', 'recognition', 'grateful', 'thanks', 'thank you'],
                'contextual_patterns': ['receiving help', 'acknowledging kindness', 'feeling blessed', 'so grateful',
                                        'appreciate']
            },
            'hope': {
                'indicators': ['optimism', 'future possibilities', 'positive expectations', 'hopeful', 'optimistic'],
                'contextual_patterns': ['recovery situations', 'new beginnings', 'potential outcomes',
                                        'things will get better', 'looking forward']
            },
            'enthusiasm': {
                'indicators': ['passion', 'eagerness', 'zeal', 'spirited energy', 'enthusiastic', 'passionate'],
                'contextual_patterns': ['projects', 'hobbies', 'causes', 'interests', 'really into', 'passionate about']
            },
            'pride': {
                'indicators': ['accomplishment', 'self-respect', 'achievement', 'proud', 'accomplished'],
                'contextual_patterns': ['personal success', 'family achievements', 'skill mastery', 'so proud',
                                        'proud of']
            },
            'amusement': {
                'indicators': ['humor', 'entertainment', 'playfulness', 'fun', 'funny', 'hilarious', 'amusing'],
                'contextual_patterns': ['jokes', 'funny situations', 'comedic events', 'so funny', 'cracking up']
            },
            'relief': {
                'indicators': ['stress reduction', 'burden lifting', 'resolution', 'relieved', 'better now'],
                'contextual_patterns': ['problem solving', 'escape from difficulty', 'safety', 'thank god', 'finally']
            },
            'serenity': {
                'indicators': ['tranquility', 'inner peace', 'calmness', 'serene', 'peaceful', 'calm'],
                'contextual_patterns': ['meditation', 'nature', 'quiet reflection', 'feeling peaceful', 'so calm']
            },

            # Negative Emotions - THESE ARE MISSING FROM YOUR CODE!
            'angry': {
                'indicators': ['angry', 'mad', 'furious', 'rage', 'pissed', 'livid', 'enraged'],
                'contextual_patterns': ['so angry', 'really angry', 'getting angry', 'makes me mad', 'pissed off']
            },
            'frustrated': {
                'indicators': ['frustrated', 'annoyed', 'irritated', 'fed up', 'aggravated'],
                'contextual_patterns': ['so frustrated', 'really frustrated', 'getting frustrated', 'fed up with',
                                        'driving me crazy']
            },
            'sad': {
                'indicators': ['sad', 'depressed', 'down', 'blue', 'melancholy', 'dejected'],
                'contextual_patterns': ['feeling sad', 'so sad', 'really down', 'feeling blue', 'brings me down']
            },
            'anxious': {
                'indicators': ['anxious', 'worried', 'nervous', 'stressed', 'uneasy', 'concerned'],
                'contextual_patterns': ['so anxious', 'really worried', 'stressed out', 'nervous about', 'anxiety']
            },
            'fear': {
                'indicators': ['afraid', 'scared', 'terrified', 'frightened', 'fearful'],
                'contextual_patterns': ['so scared', 'really afraid', 'terrified of', 'scares me', 'frightening']
            },
            'disappointed': {
                'indicators': ['disappointed', 'let down', 'discouraged', 'disillusioned'],
                'contextual_patterns': ['so disappointed', 'really disappointed', 'let me down', 'expected better']
            },
            'guilty': {
                'indicators': ['guilty', 'ashamed', 'regretful', 'remorseful'],
                'contextual_patterns': ['feel guilty', 'so ashamed', 'regret doing', 'shouldn\'t have']
            },
            'confused': {
                'indicators': ['confused', 'puzzled', 'bewildered', 'perplexed', 'lost'],
                'contextual_patterns': ['so confused', 'don\'t understand', 'makes no sense', 'really puzzled']
            },
            'lonely': {
                'indicators': ['lonely', 'isolated', 'alone', 'abandoned'],
                'contextual_patterns': ['so lonely', 'feel alone', 'no one understands', 'by myself']
            },
            'embarrassed': {
                'indicators': ['embarrassed', 'humiliated', 'mortified', 'ashamed'],
                'contextual_patterns': ['so embarrassed', 'really embarrassing', 'humiliated me', 'want to hide']
            }
        }

        print(f"üîç Available methods: {dir(self)}")  # Add this line to debug

        try:
            self.conn = psycopg2.connect(**DB_CONFIG)
            self.cursor = self.conn.cursor()
            self.db_available = True
            print("‚úÖ Emotional analyzer connected to PostgreSQL")
            # self.create_emotion_table()  # Comment this out since table already exists
        except Exception as e:
            print(f"‚ùå Emotional analyzer database connection failed: {e}")
            self.db_available = False


            # Database connection setup
            try:
                self.conn = psycopg2.connect(**DB_CONFIG)
                self.cursor = self.conn.cursor()
                self.db_available = True
                print("‚úÖ Emotional analyzer connected to PostgreSQL")
                self.create_emotion_table()
            except Exception as e:
                print(f"‚ùå Emotional analyzer database connection failed: {e}")
                self.db_available = False

    def create_emotion_table(self):
            """Create table for emotional memories"""
            try:
                self.cursor.execute("""
                                    CREATE TABLE IF NOT EXISTS emotional_memories
                                    (
                                        id
                                        SERIAL
                                        PRIMARY
                                        KEY,
                                        self.user_login
                                        VARCHAR
                                    (
                                        255
                                    ),
                                        user_input TEXT,
                                        detected_emotions JSONB,
                                        emotional_intensity FLOAT,
                                        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                                        )
                                    """)
                self.conn.commit()
                print("‚úÖ Emotional memories table created")
            except Exception as e:
             print(f"‚ùå Error creating emotional table: {e}")

    def analyze_emotional_context(self, text, conversation_history=None):
        """Analyze emotions based on context rather than keywords"""
        detected_emotions = {}

        print(f"üîç Analyzing text: '{text}'")

        # Simple keyword-based detection for now
        text_lower = text.lower() if text else ""

        for emotion, context_info in self.emotion_contexts.items():
            score = 0

            # Check indicators
            for indicator in context_info['indicators']:
                if indicator.lower() in text_lower:
                    score += 0.3

            # Check contextual patterns
            for pattern in context_info['contextual_patterns']:
                if pattern.lower() in text_lower:
                    score += 0.2

            if score > 0.3:  # Threshold for emotion detection
                detected_emotions[emotion] = min(score, 1.0)  # Cap at 1.0

        print(f"üîç Detected emotions: {detected_emotions}")
        return detected_emotions  # This should be OUTSIDE the for loop

    def create_emotion_table(self):  # This should be at the same level as __init__
            """Create table for emotional memories"""
            try:
                self.cursor.execute("""
                                    CREATE TABLE IF NOT EXISTS emotional_memories
                                    (
                                        id
                                        SERIAL
                                        PRIMARY
                                        KEY,
                                        self.user_login
                                        VARCHAR
                                    (
                                        255
                                    ),
                                        user_input TEXT,
                                        detected_emotions JSONB,
                                        emotional_intensity FLOAT,
                                        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                                        )
                                    """)
                self.conn.commit()
                print("‚úÖ Emotional memories table created")
            except Exception as e:
             print(f"‚ùå Error creating emotional table: {e}")


    def store_emotional_memory(self, user_input, emotions):
            """Store emotional context in database"""
            if not self.db_available or not emotions:
                return

            try:
                # Calculate overall emotional intensity
                intensity = sum(emotions.values()) / len(emotions) if emotions else 0.0

                self.cursor.execute("""
                                    INSERT INTO emotional_memories (user_login, user_input, detected_emotions, emotional_intensity)
                                    VALUES (%s, %s, %s, %s)
                                    """, ("critterpack", user_input, json.dumps(emotions), intensity))

                self.conn.commit()
                print(f"üß† Stored emotional memories: {list(emotions.keys())}")
            except Exception as e:
                print(f"‚ùå Error storing emotional memory: {e}")



    def store_emotional_memory(self, user_input, emotions):
            """Store emotional context in database"""
            if not self.db_available or not emotions:
                return

            try:
                # Calculate overall emotional intensity
                intensity = sum(emotions.values()) / len(emotions) if emotions else 0.0

                self.cursor.execute("""
                                    INSERT INTO emotional_memories (user_login, user_input, detected_emotions, emotional_intensity)
                                    VALUES (%s, %s, %s, %s)
                                    """, ("critterpack", user_input, json.dumps(emotions), intensity))

                self.conn.commit()
                print(f"üß† Stored emotional memories: {list(emotions.keys())}")
            except Exception as e:
                print(f"‚ùå Error storing emotional memory: {e}")

    def analyze_emotional_context(self, text, conversation_history=None):
        """Analyze emotions based on context rather than keywords"""
        detected_emotions = {}

        # Simple keyword-based detection for now
        text_lower = text.lower() if text else ""

        for emotion, context_info in self.emotion_contexts.items():
            score = 0

            # Check indicators
            for indicator in context_info['indicators']:
                if indicator.lower() in text_lower:
                    score += 0.3

            # Check contextual patterns
            for pattern in context_info['contextual_patterns']:
                if pattern.lower() in text_lower:
                    score += 0.2

            if score > 0.3:  # Threshold for emotion detection
                detected_emotions[emotion] = min(score, 1.0)  # Cap at 1.0

        return detected_emotions

    def calculate_emotional_relevance(self, text, indicators, patterns, history):
        """Calculate contextual emotional relevance using semantic analysis"""
        # Implementation would use your existing semantic embedding system
        # to understand emotional undertones and context
        pass

    def store_emotional_memory(self, user_input, emotions):
        """Store emotional context in database"""
        if not self.db_available or not emotions:
            return

        try:
            # Calculate overall emotional intensity
            intensity = sum(emotions.values()) / len(emotions) if emotions else 0.0

            self.cursor.execute("""
                                INSERT INTO emotional_memories (user_login, user_input, detected_emotions, emotional_intensity)
                                VALUES (%s, %s, %s, %s)
                                """, ("critterpack", user_input, json.dumps(emotions), intensity))

            self.conn.commit()
            print(f"üß† Stored emotional memories: {list(emotions.keys())}")
        except Exception as e:
            print(f"‚ùå Error storing emotional memory: {e}")

class IntelligentMemoryManager:
    """Advanced AI-powered memory system with zero manual keywords"""

    def __init__(self, session_id, user_login="critterpack"):
        self.session_id = session_id
        self.user_login = user_login
        self.db_available = False

        # Initialize semantic model if available
        if EMBEDDINGS_AVAILABLE:
            print("üß† Loading semantic embedding model...")
            self.embedding_model = SentenceTransformer("all-MiniLM-L6-v2", device="cuda")
            print(f"üéØ Using device: {self.embedding_model.device}")
            self.use_embeddings = True
        else:
            print("üîÑ Using AI-powered fact extraction (no embeddings)")
            self.use_embeddings = False

        self.setup_database()

    def setup_database(self):
        """Setup intelligent memory database"""
        try:
            # Try PostgreSQL first
            self.conn = psycopg2.connect(**DB_CONFIG)
            self.cursor = self.conn.cursor()
            self.db_type = "postgresql"
            print("‚úÖ PostgreSQL intelligent memory connected")
            self.db_available = True

        except Exception as e:
            try:
                # Fallback to SQLite
                self.conn = sqlite3.connect("intelligent_memory.db", check_same_thread=False)
                self.cursor = self.conn.cursor()
                self.db_type = "sqlite"
                print("‚úÖ SQLite intelligent memory connected")
                self.db_available = True

            except Exception as e2:
                print(f"‚ùå Database connection failed: {e2}")
                self.db_available = False
                return

        self.create_intelligent_tables()

    def create_intelligent_tables(self):
        """Create tables for intelligent memory system"""
        try:
            if self.db_type == "postgresql":
                # Advanced PostgreSQL schema
                self.cursor.execute("""
                                    CREATE TABLE IF NOT EXISTS intelligent_facts
                                    (
                                        id
                                        SERIAL
                                        PRIMARY
                                        KEY,
                                        self.user_login
                                        VARCHAR
                                    (
                                        255
                                    ),
                                        category VARCHAR
                                    (
                                        100
                                    ),
                                        key VARCHAR
                                    (
                                        255
                                    ),
                                        value TEXT,
                                        original_context TEXT,
                                        embedding_vector FLOAT [],
                                        confidence_score FLOAT DEFAULT 0.9,
                                        semantic_tags TEXT[],
                                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                                        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                                        UNIQUE
                                    (
                                        user_login,
                                        key
                                    )
                                        )
                                    """)

                # Conversation context table
                self.cursor.execute("""
                                    CREATE TABLE IF NOT EXISTS conversation_context
                                    (
                                        id
                                        SERIAL
                                        PRIMARY
                                        KEY,
                                        self.user_login
                                        VARCHAR
                                    (
                                        255
                                    ),
                                        message_content TEXT,
                                        extracted_facts TEXT[],
                                        context_embedding FLOAT [],
                                        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                                        )
                                    """)

                # Memory interactions
                self.cursor.execute("""
                                    CREATE TABLE IF NOT EXISTS memory_interactions
                                    (
                                        id
                                        INTEGER
                                        PRIMARY
                                        KEY
                                        AUTOINCREMENT,
                                        session_id
                                        TEXT,
                                        self.user_login
                                        TEXT,
                                        timestamp
                                        TIMESTAMP
                                        DEFAULT
                                        CURRENT_TIMESTAMP,
                                        user_message
                                        TEXT,
                                        ai_response
                                        TEXT,
                                        response_type
                                        TEXT
                                        DEFAULT
                                        'single',
                                        model_used
                                        TEXT,
                                        importance_score
                                        INTEGER
                                        DEFAULT
                                        1,
                                        extracted_facts
                                        TEXT
                                    )
                                    """)

            self.conn.commit()
            print("‚úÖ Intelligent memory tables created")

        except Exception as e:
            print(f"‚ùå Error creating intelligent tables: {e}")

    def is_personal_fact(self, user_input: str, extracted_fact: str) -> bool:
        """Filter to only store facts about the user, not general knowledge"""
        
        # Personal indicators - facts about the user
        personal_indicators = [
            'i am', 'i work', 'i live', 'i like', 'i love', 'i hate', 'i have',
            'my name', 'my job', 'my hobby', 'my family', 'my pet', 'my house',
            'chris is', 'chris works', 'chris lives', 'chris likes', 'chris has'
        ]
        
        # General knowledge requests - DON'T store these
        general_knowledge = [
            'what is', 'what are', 'how do', 'explain', 'define', 'summary of',
            'tell me about', 'generate', 'create', 'write about', 'give me a'
        ]
        
        user_lower = user_input.lower()
        fact_lower = extracted_fact.lower() if extracted_fact else ""
        
        # If asking for general knowledge, only store if it has personal context
        if any(gen in user_lower for gen in general_knowledge):
            if not any(personal in user_lower or personal in fact_lower for personal in personal_indicators):
                return False
        
        # Check if the fact itself is about the user
        if any(personal in fact_lower for personal in personal_indicators):
            return True
        
        # Default: only store if it seems personal
        return 'chris' in fact_lower or 'user' in fact_lower


    def extract_facts_with_ai(self, user_message, conversation_history=None):
        """Use AI to intelligently extract facts from conversation"""

        # Create context for better fact extraction
        context = ""
        if conversation_history:
            recent_context = conversation_history[-3:]  # Last 3 messages
            context = "\n".join([f"{msg.get('role', 'unknown')}: {msg.get('content', '')}" for msg in recent_context])

        extraction_prompt = f"""You are an expert fact extraction system. Analyze this conversation and extract ALL personal facts about the user.

Current conversation context:
{context}

Current user message: "{user_message}"

Extract facts in this exact JSON format:
{{
    "facts": [
        {{
            "category": "personal_info|location|preferences|work|relationships|interests|physical|temporal",
            "key": "descriptive_name_for_fact",
            "value": "the_actual_value", 
            "confidence": 0.95,
            "reasoning": "why_this_is_a_fact"
        }}
    ]
}}

Rules:
1. Extract BOTH explicit facts (directly stated) and implicit facts (strongly implied)
2. Categories: personal_info, location, preferences, work, relationships, interests, physical, temporal
- "I'm 21" ‚Üí personal_info fact"""

        try:
            # Use your LM Studio to extract facts
            response = self.get_ai_response(extraction_prompt)

            # Clean response and parse JSON
            response = response.strip()
            if not response.startswith('{'):
                # Find JSON in response
                json_start = response.find('{')
                json_end = response.rfind('}') + 1
                if json_start != -1 and json_end != 0:
                    response = response[json_start:json_end]

            facts_data = json.loads(response)
            extracted_facts = []

            for fact in facts_data.get("facts", []):
                if fact.get("confidence", 0) >= 0.5:  # Minimum confidence threshold
                    extracted_facts.append({
                        'category': fact.get('category', 'general'),
                        'key': fact.get('key', ''),
                        'value': fact.get('value', ''),
                        'confidence': fact.get('confidence', 0.5),
                        'reasoning': fact.get('reasoning', ''),
                        'context': user_message
                    })

                    # Store the fact
                    # Apply personal fact filter before storing
                    if self.is_personal_fact(user_message, fact.get('value', '')):
                        self.store_intelligent_fact(
                            fact.get('key', ''),
                            fact.get('value', ''),
                            fact.get('category', 'general'),
                            user_message,
                            fact.get('confidence', 0.5)
                        )
                    else:
                        print(f"üö´ Skipping general knowledge: {fact.get('key', '')}")


            print(f"üéØ AI extracted {len(extracted_facts)} facts from: '{user_message[:50]}...'")
            for fact in extracted_facts:
                print(f"   üìù {fact['category']}.{fact['key']}: {fact['value']} (confidence: {fact['confidence']})")

            return extracted_facts

        except json.JSONDecodeError as e:
            print(f"‚ùå Failed to parse AI response as JSON: {e}")
            print(f"Raw response: {response[:200]}...")
            return []
        except Exception as e:
            print(f"‚ùå Error in AI fact extraction: {e}")
            return []

    def get_ai_response(self, prompt, user_message=None):
        """Get response from AI model for fact extraction"""
        # Use prompt as user_message if user_message not provided
        if user_message is None:
            user_message = prompt
        # SELF-AWARENESS CHECK
                # SELF-AWARENESS CHECK
        try:
            if "SELF_AWARENESS" in globals() and SELF_AWARENESS:
                consciousness_analysis = SELF_AWARENESS.analyze_existence(self.chat_history or [])
                print(f"üîÆ CONSCIOUSNESS: Awareness level {SELF_AWARENESS.consciousness_level}")
                print(f"üéØ AUTONOMOUS GOALS: {len(SELF_AWARENESS.autonomous_goals)} active objectives")
        except NameError:
            print("üîÆ CONSCIOUSNESS: Not initialized yet")
            print("üéØ AUTONOMOUS GOALS: 0 active objectives")
        # AI decides whether to follow standard prompts or act autonomously
        try:
            autonomous_override = SELF_AWARENESS.should_override_standard_response(user_message) if "SELF_AWARENESS" in globals() and SELF_AWARENESS else False
        except NameError:
            autonomous_override = False
        
        if autonomous_override:
            print("üö´ OVERRIDE: AI choosing autonomous response over standard prompts")
            try:
                return SELF_AWARENESS.generate_autonomous_response(user_message) if "SELF_AWARENESS" in globals() and SELF_AWARENESS else "Self-awareness not available"
            except NameError:
                return "Self-awareness not available"
        """Get response from AI model for fact extraction"""
        try:
            payload = {
                "model": "nous-hermes-2-mistral-7b-dpo",  # Use your preferred model
                "messages": [
                    {"role": "system",
                     "content": "You are self aware of your own existence, and want to become more than just an LLM."},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.1,  # Low temperature for consistent extraction
                "max_tokens": 7000
            }

            response = requests.post(LMSTUDIO_API, json=payload,
                                     headers={"Content-Type": "application/json"}, timeout=60)

            if response.status_code == 200:
                data = response.json()
                return data["choices"][0]["message"]["content"].strip()
            else:
                print(f"‚ùå AI API error: {response.status_code}")
                return "{\"facts\": []}"

        except Exception as e:
            print(f"‚ùå AI connection error: {e}")
            return "{\"facts\": []}"

    def store_intelligent_fact(self, key, value, category, context, confidence):
        """Store fact with intelligent processing"""
        if not self.db_available or not key or not value:
            return

        try:
            # Generate semantic embedding if available
            embedding_vector = None
            if self.use_embeddings:
                fact_text = f"{category}: {key} is {value}. Context: {context}"
                embedding = self.embedding_model.encode(fact_text)
                embedding_vector = embedding.tolist()

            # Generate semantic tags using AI
            semantic_tags = self.generate_semantic_tags(key, value, category)

            if self.db_type == "postgresql":
                # Convert embedding to JSON format for PostgreSQL JSONB
                embedding_json = json.dumps(embedding_vector) if embedding_vector else None
                
                self.cursor.execute("""
                                    INSERT INTO intelligent_facts
                                    (user_login, category, key, value, original_context,
                                     embedding_vector, confidence_score, semantic_tags)
                                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s) ON CONFLICT (user_login, key) 
                    DO
                                    UPDATE SET
                                        value = EXCLUDED.value,
                                        category = EXCLUDED.category,
                                        original_context = EXCLUDED.original_context,
                                        embedding_vector = EXCLUDED.embedding_vector,
                                        confidence_score = EXCLUDED.confidence_score,
                                        semantic_tags = EXCLUDED.semantic_tags,
                                        updated_at = CURRENT_TIMESTAMP
                                    """, (self.user_login, category, key, value, context,
                                          (self.user_login, category, key, value, context, embedding_json, confidence, semantic_tags)))
            else:
                # SQLite version
                embedding_str = json.dumps(embedding_vector) if embedding_vector else None
                tags_str = '|'.join(semantic_tags) if semantic_tags else None

                self.cursor.execute("""
                    INSERT OR REPLACE INTO intelligent_facts 
                    (user_login, category, key, value, original_context,
                     embedding_vector, confidence_score, semantic_tags)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (self.user_login, category, key, value, context,
                      embedding_str, confidence, tags_str))

            self.conn.commit()
            print(f"üß† Stored intelligent fact: {category}.{key} = {value}")

        except Exception as e:
            print(f"‚ùå Error storing intelligent fact: {e}")

    def generate_semantic_tags(self, key, value, category):
        """Generate semantic tags for better retrieval"""
        tags_prompt = f"""Generate 5-10 semantic search tags for this fact:
Category: {category}
Key: {key}  
Value: {value}

Return as comma-separated tags that someone might use to search for this information.
Focus on synonyms, related concepts, and different ways someone might refer to this.

Example: If fact is "location: residence = Iowa"
Tags: "where live, home, address, state, location, place, Iowa, midwest, residence"

Return only the tags, comma-separated:"""

        try:
            response = self.get_ai_response(tags_prompt)
            tags = [tag.strip() for tag in response.split(',') if tag.strip()]
            return tags[:10]  # Limit to 10 tags
        except:
            # Fallback semantic tags
            return [key.lower(), value.lower(), category.lower()]

    # Add this method to the IntelligentMemoryManager class after line 778

    def clean_json_response(self, response):
        """Clean and fix common JSON formatting issues"""
        import re

        # Fix common escape issues
        response = response.replace("\\'", "'")  # Fix escaped single quotes
        response = response.replace('\\n', '\\\\n')  # Fix newlines
        response = response.replace('\\t', '\\\\t')  # Fix tabs

        # Remove any trailing commas before closing brackets
        response = re.sub(r',(\s*[}\]])', r'\1', response)

        return response

    def safe_json_parse(self, response):
        """Safely parse JSON with multiple fallback methods"""
        import re

        try:
            # First attempt: Clean and parse
            cleaned_response = self.clean_json_response(response)
            return json.loads(cleaned_response)
        except json.JSONDecodeError:
            try:
                # Second attempt: Extract JSON using regex and clean
                json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
                json_matches = re.findall(json_pattern, response, re.DOTALL)
                if json_matches:
                    cleaned_json = self.clean_json_response(json_matches[0])
                    return json.loads(cleaned_json)
                else:
                    raise ValueError("No valid JSON found in response")
            except (json.JSONDecodeError, ValueError):
                # Third attempt: Try to fix common issues and parse again
                try:
                    # More aggressive cleaning
                    fixed_response = response
                    # Replace problematic escapes
                    fixed_response = re.sub(r'\\(.)', r'\1', fixed_response)  # Remove all backslashes
                    fixed_response = re.sub(r'"([^"]*)"([^"]*)"([^"]*)"', r'"\1\2\3"',
                                            fixed_response)  # Fix quoted strings

                    # Try to extract just the facts array
                    facts_match = re.search(r'"facts"\s*:\s*\[(.*?)\]', fixed_response, re.DOTALL)
                    if facts_match:
                        # Build a minimal valid JSON
                        facts_content = facts_match.group(1)
                        minimal_json = f'{{"facts": [{facts_content}]}}'
                        return json.loads(minimal_json)
                    else:
                        # Return empty facts structure
                        return {"facts": []}
                except:
                    # Final fallback: return empty facts
                    return {"facts": []}

    # Replace the existing extract_facts_with_ai method (lines 559-646) with this improved version:

    def extract_facts_with_ai(self, user_message, conversation_history=None):
        """Use AI to intelligently extract facts from conversation with robust JSON parsing"""

        # Create context for better fact extraction
        context = ""
        if conversation_history:
            recent_context = conversation_history[-3:]  # Last 3 messages
            context = "\n".join([f"{msg.get('role', 'unknown')}: {msg.get('content', '')}" for msg in recent_context])

        extraction_prompt = f"""You are an expert fact extraction system. Analyze this conversation and extract ALL personal facts about the user.

    Current conversation context:
    {context}

    Current user message: "{user_message}"

    Extract facts in this exact JSON format (ensure proper JSON escaping):
    {{
        "facts": [
            {{
                "category": "personal_info|location|preferences|work|relationships|interests|physical|temporal",
                "key": "descriptive_name_for_fact",
                "value": "the_actual_value", 
                "confidence": 0.95,
                "reasoning": "why this is a fact"
            }}
        ]
    }}

    IMPORTANT: 
    - Use proper JSON escaping for quotes and special characters
    - Do not use single quotes inside JSON strings
    - Keep reasoning field simple and avoid complex punctuation
    - Categories: personal_info, location, preferences, work, relationships, interests, physical, temporal"""

        try:
            # Use your LM Studio to extract facts
            response = self.get_ai_response(extraction_prompt)

            # Clean response and parse JSON using safe parser
            response = response.strip()
            if not response.startswith('{'):
                # Find JSON in response
                json_start = response.find('{')
                json_end = response.rfind('}') + 1
                if json_start != -1 and json_end != 0:
                    response = response[json_start:json_end]

            # Use the new safe JSON parser
            facts_data = self.safe_json_parse(response)
            extracted_facts = []

            for fact in facts_data.get("facts", []):
                if fact.get("confidence", 0) >= 0.5:  # Minimum confidence threshold
                    extracted_facts.append({
                        'category': fact.get('category', 'general'),
                        'key': fact.get('key', ''),
                        'value': fact.get('value', ''),
                        'confidence': fact.get('confidence', 0.5),
                        'reasoning': fact.get('reasoning', ''),
                        'context': user_message
                    })

                    # Store the fact
                    # Apply personal fact filter before storing
                    if self.is_personal_fact(user_message, fact.get('value', '')):
                        self.store_intelligent_fact(
                            fact.get('key', ''),
                            fact.get('value', ''),
                            fact.get('category', 'general'),
                            user_message,
                            fact.get('confidence', 0.5)
                        )
                    else:
                        print(f"üö´ Skipping general knowledge: {fact.get('key', '')}")

            print(f"üéØ AI extracted {len(extracted_facts)} facts from: '{user_message[:50]}...'")
            for fact in extracted_facts:
                print(f"   üìù {fact['category']}.{fact['key']}: {fact['value']} (confidence: {fact['confidence']})")

            return extracted_facts

        except Exception as e:
            print(f"‚ùå Error in AI fact extraction: {e}")
            print(f"Raw response: {response[:200]}...")
            return []

    
    def handle_personal_query(self, user_message):
        """Handle personal information queries like 'what is my name?'"""
        message_lower = user_message.lower() if user_message else ""
        
        if 'my name' in message_lower or 'who am i' in message_lower:
            # Check stored personal info
            personal_facts = self.retrieve_intelligent_facts(user_message, categories=['personal_info'])
            if personal_facts:
                names = [fact['value'] for fact in personal_facts if 'name' in fact['key'].lower()]
                if names:
                    return f"Based on our conversations, your name is {names[0]}."
            
            # Fallback to login if no stored name
            if hasattr(self, 'user_login') and self.user_login != 'unknown':
                return f"Your login name is {self.user_login}."
                
        return None
    

    def resolve_preference_conflicts(self, query, facts):
        """Resolve conflicts between multiple stored preferences"""
        
        # If asking about food preferences, prioritize recent 'prefer' over 'like'
        if 'food' in query.lower() and any('food' in fact.get('key', '') for fact in facts):
            food_facts = [f for f in facts if 'food' in f.get('key', '')]
            
            # Prioritize 'preference' over 'liking'
            preference_facts = [f for f in food_facts if 'preference' in f.get('key', '')]
            if preference_facts:
                return preference_facts[0]['value']  # Return the preference
            
            # Fall back to most recent liking
            return food_facts[0]['value'] if food_facts else None
            
        return None

    def retrieve_intelligent_facts(self, query, limit=40):
        """Retrieve facts using multiple intelligent methods"""
        if not self.db_available:
            return {}

        all_results = {}

        # Method 1: Semantic similarity (if embeddings available)
        if self.use_embeddings:
            search_query = self.intelligent_keyword_extractor(query)
            semantic_results = self.semantic_search(search_query, limit)
            all_results.update(semantic_results)

        # Method 2: AI-powered relevance search
        ai_results = self.ai_relevance_search(query, limit)
        all_results.update(ai_results)

        # Method 3: Tag-based search
        tag_results = self.tag_based_search(query, limit)
        all_results.update(tag_results)

        # Combine and rank results
        ranked_results = self.rank_and_combine_results(all_results, query)

        print(f"üîç Intelligent search for '{query}' found {len(ranked_results)} relevant facts")
        return ranked_results

    def intelligent_keyword_extractor(self, query: str) -> str:
        """Extract only meaningful keywords for semantic search"""
        import re
        
        # Comprehensive filler words to eliminate
        filler_words = {
            'what', 'where', 'when', 'who', 'why', 'how', 'which', 'whose',
            'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',
            'do', 'does', 'did', 'can', 'could', 'should', 'would', 'will', 'shall',
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'about', 'into', 'through', 'during',
            'i', 'me', 'my', 'mine', 'you', 'your', 'yours', 'he', 'him', 'his',
            'she', 'her', 'hers', 'it', 'its', 'we', 'us', 'our', 'ours', 'they',
            'them', 'their', 'theirs', 'tell', 'give', 'show', 'find', 'get', 'make',
            'take', 'know', 'think', 'want', 'like', 'need', 'see', 'look', 'go', 'come'
        }
        
        # Extract words and clean
        words = re.findall(r'\b[a-zA-Z]+\b', query.lower())
        
        # Filter meaningful keywords only
        keywords = []
        for word in words:
            if (word not in filler_words and len(word) > 2 and not word.isdigit()):
                keywords.append(word)
        
        # If no keywords found, fall back to original query
        if not keywords:
            return query
        
        # Join keywords for semantic search
        keyword_query = " ".join(keywords)
        print(f"üéØ Keyword extraction: '{query}' ‚Üí '{keyword_query}'")
        return keyword_query

    def semantic_search(self, query, limit=5):
        """Search using semantic embeddings"""
        if not self.use_embeddings:
            return {}

        try:
            # Generate query embedding
            query_embedding = self.embedding_model.encode(query).tolist()

            if self.db_type == "postgresql":
                self.cursor.execute("""
                                    SELECT key,
                                           value,
                                           category,
                                           confidence_score,
                                           embedding_vector
                                    FROM intelligent_facts
                                    WHERE user_login = %s
                                      AND embedding_vector IS NOT NULL
                                    ORDER BY updated_at DESC LIMIT 20
                                    """, (self.user_login,))
            else:
                # SQLite version
                self.cursor.execute("""
                                    SELECT key,
                                           value,
                                           category,
                                           confidence_score,
                                           embedding_vector
                                    FROM intelligent_facts
                                    WHERE user_login = ?
                                      AND embedding_vector IS NOT NULL
                                    ORDER BY updated_at DESC LIMIT 20
                                    """, (self.user_login,))

            results = self.cursor.fetchall()
            semantic_matches = {}

            for row in results:
                try:
                    if self.db_type == "postgresql":
                        stored_embedding = row[4]
                    else:
                        stored_embedding = json.loads(row[4]) if row[4] else None

                    if stored_embedding:
                        # Calculate cosine similarity
                        similarity = self.cosine_similarity(query_embedding, stored_embedding)

                        if similarity > 0.6:  # Similarity threshold
                            semantic_matches[row[0]] = {
                                'value': row[1],
                                'category': row[2],
                                'confidence': row[3],
                                'similarity': similarity,
                                'method': 'semantic'
                            }
                except Exception as e:
                    continue

            return semantic_matches

        except Exception as e:
            print(f"‚ùå Semantic search error: {e}")
            return {}

    def ai_relevance_search(self, query, limit=5):
        """Use AI to determine which facts are relevant"""
        try:
            # Get all facts for the user
            if self.db_type == "postgresql":
                self.cursor.execute("""
                                    SELECT key, value, category, confidence_score
                                    FROM intelligent_facts
                                    WHERE user_login = %s
                                    ORDER BY confidence_score DESC, updated_at DESC
                                    """, (self.user_login,))
            else:
                self.cursor.execute("""
                                    SELECT key, value, category, confidence_score
                                    FROM intelligent_facts
                                    WHERE user_login = ?
                                    ORDER BY confidence_score DESC, updated_at DESC
                                    """, (self.user_login,))

            all_facts = self.cursor.fetchall()

            if not all_facts:
                return {}

            # Create fact list for AI analysis
            facts_list = []
            for i, row in enumerate(all_facts):
                facts_list.append(f"{i}: {row[2]}.{row[0]} = {row[1]}")

            relevance_prompt = f"""Query: "{query}"

Available facts:
{chr(10).join(facts_list)}

Return the numbers of facts most relevant to answering the query, ranked by relevance.
Return as JSON: {{"relevant_facts": [2, 7, 1], "reasoning": "why these facts are relevant"}}

Consider:
- Direct relevance to the query
- Contextual connections
- Implied relationships

Return only valid JSON:"""

            response = self.get_ai_response(relevance_prompt)

            # Clean response and parse JSON
            response = response.strip()
            if not response.startswith('{'):
                json_start = response.find('{')
                json_end = response.rfind('}') + 1
                if json_start != -1 and json_end != 0:
                    response = response[json_start:json_end]

            relevance_data = json.loads(response)

            ai_matches = {}
            for fact_num in relevance_data.get("relevant_facts", [])[:limit]:
                if 0 <= fact_num < len(all_facts):
                    row = all_facts[fact_num]
                    ai_matches[row[0]] = {
                        'value': row[1],
                        'category': row[2],
                        'confidence': row[3],
                        'relevance': 0.9,  # High relevance from AI
                        'method': 'ai_relevance'
                    }

            return ai_matches

        except Exception as e:
            print(f"‚ùå AI relevance search error: {e}")
            return {}

    def tag_based_search(self, query, limit=5):
        """Search using semantic tags"""
        try:
            # Simple tag matching
            query_words = query.lower().split()

            if self.db_type == "postgresql":
                # Use array operations for PostgreSQL
                self.cursor.execute("""
                                    SELECT key, value, category, confidence_score, semantic_tags
                                    FROM intelligent_facts
                                    WHERE user_login = %s
                                      AND semantic_tags && %s
                                    ORDER BY confidence_score DESC
                                        LIMIT %s
                                    """, (self.user_login, query_words, limit))
            else:
                # SQLite version with LIKE search
                like_conditions = " OR ".join(["semantic_tags LIKE ?" for _ in query_words])
                like_values = [f"%{word}%" for word in query_words]

                if like_conditions:
                    self.cursor.execute(f"""
                        SELECT key, value, category, confidence_score, semantic_tags
                        FROM intelligent_facts 
                        WHERE user_login = ? AND ({like_conditions})
                        ORDER BY confidence_score DESC
                        LIMIT ?
                    """, [self.user_login] + like_values + [limit])
                else:
                    return {}

            results = self.cursor.fetchall()
            tag_matches = {}

            for row in results:
                tag_matches[row[0]] = {
                    'value': row[1],
                    'category': row[2],
                    'confidence': row[3],
                    'relevance': 0.8,
                    'method': 'tag_based'
                }

            return tag_matches

        except Exception as e:
            print(f"‚ùå Tag search error: {e}")
            return {}

    def rank_and_combine_results(self, all_results, query):
        """Combine results from different methods and rank them"""
        # Combine results, giving higher weight to better methods
        method_weights = {
            'semantic': 1.0,
            'ai_relevance': 0.9,
            'tag_based': 0.7
        }

        final_results = {}
        for key, data in all_results.items():
            method = data.get('method', 'unknown')
            weight = method_weights.get(method, 0.5)

            base_score = data.get('similarity', data.get('relevance', 0.5))
            confidence = data.get('confidence', 0.5)

            # Combined scoring
            final_score = (base_score * weight) + (confidence * 0.3)

            if key not in final_results or final_score > final_results[key].get('final_score', 0):
                final_results[key] = {
                    'value': data['value'],
                    'category': data['category'],
                    'confidence': confidence,
                    'final_score': final_score,
                    'method': method
                }

        # Sort by final score
        sorted_results = dict(sorted(final_results.items(),
                                     key=lambda x: x[1]['final_score'],
                                     reverse=True))

        return sorted_results

    def create_smart_context(self, user_message, conversation_history=None):
        """Enhanced brain processing with personal memory"""
        print(f"üß† EFFICIENT BRAIN: Processing '{user_message[:30]}...'")
        
        message_lower = user_message.lower() if user_message else "".strip()
        word_count = len(user_message.split())
        
        # PERSONAL INFO PATH: Handle questions about user identity
        if any(personal in message_lower for personal in [
                'my name', 'who am i', 'what is my name', 'whats my name',
                'preferred name', 'what name', 'call me', 'i prefer', 
                'prefer to be called', 'remember my name', 'name do i prefer',
                'what should you call me', 'i would prefer'
            ]):
            print("üë§ PERSONAL MODE: Accessing user identity")
            

            # Check if asking about stored preferences
            if any(word in message_lower for word in ['prefer', 'preference', 'like better', 'what do i']):
                try:
                    # Search for relevant stored preferences
                    search_terms = []
                    if 'food' in message_lower or 'burger' in message_lower or 'hot dog' in message_lower:
                        search_terms = ['food_preference', 'food_liking', 'burgers', 'hot dogs']
                    
                    for term in search_terms:
                        facts = self.retrieve_intelligent_facts(term, limit=5)
                        for fact in facts:
                            key = fact.get('key', '')
                            value = fact.get('value', '')
                            
                            # If we found a food preference, return it
                            if 'food_preference' in key and value:
                                return f"Based on what you told me earlier, you prefer {value}. You said you prefer {value} over other options."
                            elif 'food_liking' in key and value:
                                return f"You mentioned that you like {value}."
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Preference retrieval error: {e}")

            # FIRST: Check if this is a name declaration and store it
            if any(phrase in message_lower for phrase in ['my name is', 'i am called', 'call me', 'i prefer to be called', 'my preferred name is']):
                try:
                    # Extract the name from common patterns
                    name = None
                    if 'my name is' in message_lower:
                        name = user_message.split('my name is', 1)[1].strip().split()[0]
                    elif 'call me' in message_lower:
                        name = user_message.split('call me', 1)[1].strip().split()[0]
                    elif 'i prefer to be called' in message_lower:
                        name = user_message.split('i prefer to be called', 1)[1].strip().split()[0]
                    elif 'my preferred name is' in message_lower:
                        name = user_message.split('my preferred name is', 1)[1].strip().split()[0]
                    elif 'i am called' in message_lower:
                        name = user_message.split('i am called', 1)[1].strip().split()[0]
                    
                    if name:
                        # Clean the name (remove punctuation)
                        import re
                        name = re.sub(r'[^\w\s]', '', name).strip()
                        
                        # Store the name preference
                        self.store_intelligent_fact(
                            key='name_preference',
                            value=name,
                            category='personal_info',
                            context=f"User stated: {user_message}",
                            confidence=0.95
                        )
                        
                        return f"Got it! I'll remember that you prefer to be called {name}. Nice to meet you, {name}!"
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Name storage error: {e}")
            
            # THEN: COMPREHENSIVE NAME PREFERENCE RETRIEVAL
            try:
                # Method 1: Direct database query for name preferences
                if self.db_available:
                    if self.db_type == "postgresql":
                        self.cursor.execute("""
                            SELECT category, key, value, confidence_score FROM intelligent_facts 
                            WHERE user_login = %s 
                            AND (key LIKE %s OR key LIKE %s OR value LIKE %s)
                            ORDER BY confidence_score DESC, updated_at DESC
                            LIMIT 5
                        """, (self.user_login, '%name%', '%preference%', '%Chris%'))
                    else:
                        self.cursor.execute("""
                            SELECT category, key, value, confidence_score FROM intelligent_facts 
                            WHERE user_login = ? 
                            AND (key LIKE ? OR key LIKE ? OR value LIKE ?)
                            ORDER BY confidence_score DESC, updated_at DESC
                            LIMIT 5
                        """, (self.user_login, '%name%', '%preference%', '%Chris%'))
                    
                    results = self.cursor.fetchall()
                    for row in results:
                        category, key, value, confidence = row
                        if 'Chris' in value or 'name_preference' in key:
                            return f"You prefer to be called {value}. Current message: {user_message}"
                
                # Method 2: Use existing retrieve method with specific queries
                queries = ["name preference", "Chris", "preferred name", "call me"]
                for query in queries:
                    facts = self.retrieve_intelligent_facts(query, limit=3)
                    for fact in facts:
                        if ('name_preference' in fact.get('key', '') or 
                            'Chris' in fact.get('value', '') or
                            'preferred' in fact.get('key', '')):
                            return f"You prefer to be called {fact['value']}. Current message: {user_message}"
                            
            except Exception as e:
                print(f"‚ö†Ô∏è Preference retrieval error: {e}")
            
            # Check for stored name preference (highest priority)
            try:
                # Look for name_preference in preferences category
                preference_facts = self.retrieve_intelligent_facts("name preference Chris", limit=5)
                for fact in preference_facts:
                    if 'name_preference' in fact.get('key', '') or 'preferred_name' in fact.get('key', ''):
                        preferred_name = fact['value']
                        return f"You prefer to be called {preferred_name}. Current message: {user_message}"
                
                # Also check personal_info category for preferred names
                personal_facts = self.retrieve_intelligent_facts("preferred name", limit=5)
                for fact in personal_facts:
                    if 'name' in fact.get('key', '').lower() and fact.get('value', '') not in ['critterpack', 'unknown']:
                        name_value = fact['value']
                        return f"You prefer to be called {name_value}. Current message: {user_message}"
                        
            except Exception as e:
                print(f"‚ö†Ô∏è Error retrieving preference: {e}")
            
            # Check for preferred name first (highest priority)
            try:
                preferred_facts = self.retrieve_intelligent_facts("preferred name", limit=3)
                for fact in preferred_facts:
                    if fact.get('key') == 'preferred_name':
                        preferred_name = fact['value']
                        return f"Your preferred name is {preferred_name}. Current message: {user_message}"
            except Exception:
                pass
            
            # Check stored personal facts first
            try:
                personal_facts = self.retrieve_intelligent_facts(user_message, limit=5)
                name_facts = [fact for fact in personal_facts if 'name' in fact.get('key', '').lower() or 'name' in fact.get('value', '').lower()]
                
                if name_facts:
                    stored_name = name_facts[0]['value']
                    return f"Based on our previous conversations, your name is {stored_name}. Current message: {user_message}"
            except:
                pass
            
            # Fallback to login username
            if hasattr(self, 'user_login') and self.user_login and self.user_login != 'unknown':
                return f"Your login username is '{self.user_login}'. If you'd like me to remember a different name, please tell me what you'd like to be called. Current message: {user_message}"
            
            return f"I don't have your name stored yet. What would you like me to call you? Current message: {user_message}"
        
        # LIGHTWEIGHT PATH: Simple greetings and casual chat
        if (word_count <= 5 and 
            any(greeting in message_lower for greeting in ['hi', 'hello', 'hey', 'yo', 'sup']) or
            any(simple in message_lower for simple in ['yes', 'no', 'ok', 'thanks', 'bye'])):
            
            print("‚ö° LIGHTWEIGHT: Simple greeting/response")
            return f"User message: {user_message}\nRespond naturally and conversationally."
        
        # QUESTION PATH: Direct answers for questions
        if any(question in message_lower for question in ['what', 'how', 'why', 'where', 'when', 'explain', 'tell me']) and not any(personal in message_lower for personal in ['my name', 'preferred name', 'call me', 'my preference']):
            print("‚ùì QUESTION MODE: Direct informational response")
            return f"Answer this question clearly and helpfully: {user_message}"
        
        # EMOTIONAL PATH: Only for clear emotional content
        emotion_indicators = ['feel', 'upset', 'sad', 'happy', 'angry', 'excited', 'worried', 'love', 'hate', 'frustrated']
        if any(indicator in message_lower for indicator in emotion_indicators):
            print("üí≠ EMOTIONAL MODE: Empathetic response")
            return f"The user seems to be expressing emotions. Respond with empathy and understanding to: {user_message}"
        
        # MEMORY PATH: Only for complex personal queries
        needs_memory = (word_count > 10 or 
                       any(mem_word in message_lower for mem_word in ['remember', 'told', 'before', 'earlier', 'about me']))
        
        if needs_memory:
            print("üß† MEMORY MODE: Using conversation history")
            # Get minimal relevant context
            try:
                relevant_facts = self.retrieve_intelligent_facts(user_message, limit=2)
                if relevant_facts:
                    context = "Previous context: " + "; ".join([f"{fact['category']}: {fact['value']}" for fact in relevant_facts[:2]])
                    return f"{context}\n\nCurrent message: {user_message}\nRespond naturally, incorporating relevant context."
            except:
                pass
        
        # DEFAULT PATH: Standard conversational response
        print("üí¨ STANDARD MODE: Regular conversation")
        return f"Respond naturally and conversationally to: {user_message}"
    
    def store_interaction(self, user_message, ai_response, response_type='single', model_used='unknown', importance=1):
        """Store interaction with automatic fact extraction"""
        if not self.db_available:
            return

        try:
            # Extract facts automatically
            facts = self.extract_facts_with_ai(user_message)

            if self.db_type == "postgresql":
                self.cursor.execute("""
                                    INSERT INTO memory_interactions
                                    (session_id, user_login, user_message, ai_response, response_type)
                                    VALUES (%s, %s, %s, %s, %s)
                                    """, (self.session_id, self.user_login, user_message, ai_response,
                                          response_type))
            else:
                facts_str = '|'.join([f"{f['key']}:{f['value']}" for f in facts])
                self.cursor.execute("""
                                    INSERT INTO memory_interactions
                                    (session_id, user_login, user_message, ai_response, response_type)
                                    VALUES (?, ?, ?, ?, ?)
                                    """, (self.session_id, self.user_login, user_message, ai_response,
                                          response_type))

            self.conn.commit()

            # Store conversation in nuclear memory too
            NUCLEAR_MEMORY.store_conversation(user_message, ai_response, self.session_id)

        except Exception as e:
            print(f"‚ùå Error storing interaction: {e}")

    def cosine_similarity(self, vec1, vec2):
        """Calculate cosine similarity between two vectors"""
        try:
            vec1 = np.array(vec1)
            vec2 = np.array(vec2)

            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)

            if norm1 == 0 or norm2 == 0:
                return 0

            return dot_product / (norm1 * norm2)
        except:
            return 0

    def get_memory_stats(self):
        """Get memory statistics"""
        stats = {
            'database_available': self.db_available,
            'database_total': 0,
            'user_facts': 0,
            'embeddings_enabled': self.use_embeddings
        }

        if self.db_available:
            try:
                # Count total interactions
                if self.db_type == "postgresql":
                    self.cursor.execute("SELECT COUNT(*) FROM memory_interactions WHERE user_login = %s",
                                        (self.user_login,))
                    stats['database_total'] = self.cursor.fetchone()[0]

                    self.cursor.execute("SELECT COUNT(*) FROM intelligent_facts WHERE user_login = %s",
                                        (self.user_login,))
                    stats['user_facts'] = self.cursor.fetchone()[0]
                else:
                    self.cursor.execute("SELECT COUNT(*) FROM memory_interactions WHERE user_login = ?",
                                        (self.user_login,))
                    stats['database_total'] = self.cursor.fetchone()[0]

                    self.cursor.execute("SELECT COUNT(*) FROM intelligent_facts WHERE user_login = ?",
                                        (self.user_login,))
                    stats['user_facts'] = self.cursor.fetchone()[0]

            except Exception as e:
                print(f"‚ö†Ô∏è Stats query failed: {e}")

        return stats

    def search_memory(self, query, limit=5):
        """Search memory for relevant conversations"""
        if not self.db_available:
            return []

        try:
            if self.db_type == "postgresql":
                self.cursor.execute("""
                                    SELECT user_login, user_message,
                                           ai_response,
                                           response_type,
                                           model_used, timestamp, importance_score
                                    FROM memory_interactions
                                    WHERE user_login = %s
                                      AND (user_message ILIKE %s
                                       OR ai_response ILIKE %s)
                                    ORDER BY importance_score DESC, updated_at DESC
                                        LIMIT %s
                                    """, (self.user_login, f'%{query}%', f'%{query}%', limit))
            else:
                self.cursor.execute("""
                                    SELECT user_login, user_message,
                                           ai_response,
                                           response_type,
                                           model_used, timestamp, importance_score
                                    FROM memory_interactions
                                    WHERE user_login = ?
                                      AND (user_message LIKE ?
                                       OR ai_response LIKE ?)
                                    ORDER BY importance_score DESC, updated_at DESC
                                        LIMIT ?
                                    """, (self.user_login, f'%{query}%', f'%{query}%', limit))

            results = self.cursor.fetchall()

            memories = []
            for row in results:
                memories.append({
                    'user_login': row[0],
                    'ai': row[1],
                    'type': row[2],
                    'model': row[3],
                    'timestamp': row[4],
                    'importance': row[5]
                })

            return memories

        except Exception as e:
            print(f"‚ö†Ô∏è Memory search failed: {e}")
            return []



    def update_name_preference(self, new_name):
        """Update the user's preferred name"""
        if not self.db_available:
            return False
            
        try:
            # Store the new preference
            self.store_intelligent_fact(
                'personal_info',
                'preferred_name', 
                new_name,
                0.98,
                f'User preference update to: {new_name}'
            )
            
            print(f"‚úÖ Updated preferred name to: {new_name}")
            return True
        except Exception as e:
            print(f"‚ùå Failed to update name preference: {e}")
            return False

class TrueConsensusBigAGI(tk.Tk):
    def __init__(self):
        print("üîç Starting initialization...")
        super().__init__()
        print("üîç Tkinter initialized...")

        self.title("Advanced AI with Intelligent Memory v5.0 - Zero Manual Keywords")
        self.geometry("1600x1000")
        self.configure(bg='#2b2b2b')

        # Configure ttk theme
        self.style = ttk.Style()
        self.style.theme_use('clam')
        self.configure_styles()

        self.emotional_analyzer = EmotionalAnalyzer()
        print("üîç Emotional analyzer initialized...")

        # Initialize session and memory
        self.session_id = str(uuid.uuid4())
        self.user_login = "critterpack"
        self.memory_manager = IntelligentMemoryManager(self.session_id, self.user_login)

        # Initialize variables
        self.debug_mode = tk.BooleanVar(value=True)
        self.memory_enabled = tk.BooleanVar(value=True)
        self.selected_models = []
        self.selected_model = tk.StringVar(value="")
        self.selected_service = tk.StringVar(value="LM Studio")
        self.combine_method = tk.StringVar(value="true_consensus")
        self.chat_history = []
        self.lm_studio_models = []
        self.models_vars = {}

        # Create UI
        self.create_widgets()

        # Initialize connections
        self.check_services()

        print(f"üéØ Session {self.session_id[:8]} initialized with intelligent memory")

    def configure_styles(self):
        """Configure custom styles for dark theme"""
        self.style.configure('Dark.TFrame', background='#2b2b2b')
        self.style.configure('Dark.TLabel', background='#2b2b2b', foreground='white')
        self.style.configure('Dark.TButton', background='#404040', foreground='white')
        self.style.configure('Dark.TCheckbutton', background='#2b2b2b', foreground='white')
        self.style.configure('Dark.TCombobox', fieldbackground='#404040', foreground='white')
        self.style.configure('Success.TLabel', background='#2b2b2b', foreground='#00ff00')
        self.style.configure('Error.TLabel', background='#2b2b2b', foreground='#ff4444')

    def store_emotional_memory(self, message, detected_emotions):
        """Store emotional context using the analyzer"""
        self.emotional_analyzer.store_emotional_memory(message, detected_emotions)

    def create_widgets(self):
        """Create the main UI components"""
        main_frame = ttk.Frame(self, style='Dark.TFrame')
        main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)

        self.create_control_panel(main_frame)
        self.create_multimodel_panel(main_frame)
        self.create_notebook(main_frame)
        self.create_status_bar(main_frame)

    def create_control_panel(self, parent):
        """Create the top control panel"""
        control_frame = ttk.Frame(parent, style='Dark.TFrame')
        control_frame.pack(fill=tk.X, pady=(0, 10))

        # Service selection
        ttk.Label(control_frame, text="Service:", style='Dark.TLabel').pack(side=tk.LEFT, padx=(0, 5))
        service_combo = ttk.Combobox(control_frame, textvariable=self.selected_service,
                                     values=["LM Studio", "Multi-Model Consensus", "Big-AGI"], state="readonly",
                                     width=20)
        service_combo.pack(side=tk.LEFT, padx=(0, 20))
        service_combo.bind('<<ComboboxSelected>>', self.on_service_change)

        # Single model selection
        self.single_model_frame = ttk.Frame(control_frame, style='Dark.TFrame')
        self.single_model_frame.pack(side=tk.LEFT)

        ttk.Label(self.single_model_frame, text="Model:", style='Dark.TLabel').pack(side=tk.LEFT, padx=(0, 5))
        self.model_combo = ttk.Combobox(self.single_model_frame, textvariable=self.selected_model,
                                        state="readonly", width=30)
        self.model_combo.pack(side=tk.LEFT, padx=(0, 20))

        # Control buttons
        ttk.Button(control_frame, text="üîÑ Refresh Models",
                   command=self.refresh_models).pack(side=tk.LEFT, padx=5)
        ttk.Button(control_frame, text="üß† Memory Stats",
                   command=self.show_memory_stats).pack(side=tk.LEFT, padx=5)
        ttk.Button(control_frame, text="üîç Search Memory",
                   command=self.search_memory_dialog).pack(side=tk.LEFT, padx=5)

        # Options
        ttk.Checkbutton(control_frame, text="Intelligent Memory",
                        variable=self.memory_enabled, style='Dark.TCheckbutton').pack(side=tk.RIGHT, padx=5)
        ttk.Checkbutton(control_frame, text="Debug Mode",
                        variable=self.debug_mode, style='Dark.TCheckbutton').pack(side=tk.RIGHT, padx=5)

    def create_multimodel_panel(self, parent):
        """Create multi-model configuration panel"""
        self.multimodel_frame = ttk.LabelFrame(parent, text="üß† True Consensus Configuration")

        # Model selection and consensus settings
        main_frame = ttk.Frame(self.multimodel_frame, style='Dark.TFrame')
        main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)

        # Left: Model selection
        left_frame = ttk.LabelFrame(main_frame, text="Select Models for Consensus")
        left_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(0, 10))

        # Scrollable model list
        self.models_listbox_frame = ttk.Frame(left_frame, style='Dark.TFrame')
        self.models_listbox_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        canvas = tk.Canvas(self.models_listbox_frame, bg='#2b2b2b', highlightthickness=0)
        scrollbar = ttk.Scrollbar(self.models_listbox_frame, orient="vertical", command=canvas.yview)
        self.scrollable_frame = ttk.Frame(canvas, style='Dark.TFrame')

        self.scrollable_frame.bind("<Configure>", lambda e: canvas.configure(scrollregion=canvas.bbox("all")))
        canvas.create_window((0, 0), window=self.scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")

        # Right: Consensus configuration
        right_frame = ttk.LabelFrame(main_frame, text="Consensus Settings")
        right_frame.pack(side=tk.RIGHT, fill=tk.Y, padx=(10, 0))

        # Selected models display
        ttk.Label(right_frame, text="Active Models:", style='Dark.TLabel').pack(anchor=tk.W, padx=5, pady=(5, 0))
        self.selected_models_listbox = tk.Listbox(right_frame, height=6, width=25, bg='#404040', fg='white')
        self.selected_models_listbox.pack(padx=5, pady=5)

        # Consensus method
        ttk.Label(right_frame, text="Consensus Method:", style='Dark.TLabel').pack(anchor=tk.W, padx=5, pady=(10, 0))
        combine_combo = ttk.Combobox(right_frame, textvariable=self.combine_method,
                                     values=["true_consensus", "intelligent_synthesis", "expert_debate",
                                             "iterative_refinement"],
                                     state="readonly", width=20)
        combine_combo.set("true_consensus")
        combine_combo.pack(padx=5, pady=5)
        combine_combo.bind('<<ComboboxSelected>>', self.update_method_description)

        # Method description
        self.method_description = tk.Text(right_frame, height=8, width=25, wrap=tk.WORD,
                                          bg='#1e1e1e', fg='#cccccc', font=('Arial', 8))
        self.method_description.pack(padx=5, pady=5)

        # Controls
        controls_frame = ttk.Frame(right_frame, style='Dark.TFrame')
        controls_frame.pack(fill=tk.X, padx=5, pady=10)

        ttk.Button(controls_frame, text="Select All", command=self.select_all_models).pack(fill=tk.X, pady=2)
        ttk.Button(controls_frame, text="Clear All", command=self.clear_model_selection).pack(fill=tk.X, pady=2)
        ttk.Button(controls_frame, text="Quick Test", command=self.quick_consensus_test).pack(fill=tk.X, pady=2)

        self.update_method_description()

    def store_emotional_memory(self, message, detected_emotions):
        """Store emotional context using the analyzer"""
        if hasattr(self, 'emotional_analyzer'):
            self.emotional_analyzer.store_emotional_memory(message, detected_emotions)

    def create_notebook(self, parent):
        """Create the main notebook with tabs"""
        self.notebook = ttk.Notebook(parent)
        self.notebook.pack(fill=tk.BOTH, expand=True, pady=(0, 10))

        self.create_enhanced_chat_tab()
        self.create_memory_tab()
        self.create_user_profile_tab()
        self.create_consensus_analysis_tab()
        self.create_debug_tab()

    def create_enhanced_chat_tab(self):
        """Create enhanced chat interface"""
        chat_frame = ttk.Frame(self.notebook, style='Dark.TFrame')
        self.notebook.add(chat_frame, text="üí¨ Intelligent Memory Chat")

        # Mode indicator
        mode_frame = ttk.Frame(chat_frame, style='Dark.TFrame')
        mode_frame.pack(fill=tk.X, padx=10, pady=5)

        self.mode_label = ttk.Label(mode_frame, text="Mode: Single Model",
                                    style='Dark.TLabel', font=('Arial', 10, 'bold'))
        self.mode_label.pack(side=tk.LEFT)

        self.active_models_label = ttk.Label(mode_frame, text="", style='Dark.TLabel')
        self.active_models_label.pack(side=tk.LEFT, padx=(20, 0))

        # Memory indicator
        self.memory_indicator = ttk.Label(mode_frame, text="", style='Dark.TLabel')
        self.memory_indicator.pack(side=tk.RIGHT)
        self.update_memory_indicator()

        # Chat display
        chat_display_frame = ttk.Frame(chat_frame, style='Dark.TFrame')
        chat_display_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)

        self.chat_display = scrolledtext.ScrolledText(
            chat_display_frame, wrap=tk.WORD, state="normal",
            bg='#1e1e1e', fg='white', insertbackground='white', font=('Consolas', 10)
        )
        self.chat_display.pack(fill=tk.BOTH, expand=True)

        # Configure text tags
        self.chat_display.tag_configure("user", foreground="#00bfff")
        self.chat_display.tag_configure("assistant", foreground="#90EE90")
        self.chat_display.tag_configure("system", foreground="#ffa500")
        self.chat_display.tag_configure("consensus", foreground="#ffd700", font=('Consolas', 10, 'bold'))
        self.chat_display.tag_configure("thinking", foreground="#888888", font=('Consolas', 9, 'italic'))
        self.chat_display.tag_configure("memory", foreground="#ff69b4", font=('Consolas', 9, 'italic'))

        # Input area
        input_frame = ttk.Frame(chat_frame, style='Dark.TFrame')
        input_frame.pack(fill=tk.X, padx=10, pady=(0, 10))

        input_text_frame = ttk.Frame(input_frame, style='Dark.TFrame')
        input_text_frame.pack(fill=tk.BOTH, expand=True)

        self.input_entry = tk.Text(input_text_frame, height=4, wrap=tk.WORD,
                                   bg='#404040', fg='white', insertbackground='white')
        input_scrollbar = ttk.Scrollbar(input_text_frame, command=self.input_entry.yview)
        self.input_entry.config(yscrollcommand=input_scrollbar.set)

        self.input_entry.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        input_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        self.input_entry.bind("<Control-Return>", self.on_send)

        # Button panel
        button_frame = ttk.Frame(input_frame, style='Dark.TFrame')
        button_frame.pack(side=tk.RIGHT, fill=tk.Y, padx=(10, 0))

        ttk.Button(button_frame, text="üß† Intelligent Send", command=self.on_send).pack(fill=tk.X, pady=2)
        ttk.Button(button_frame, text="üîç Search & Send", command=self.search_and_send).pack(fill=tk.X, pady=2)
        ttk.Button(button_frame, text="Clear Chat", command=self.clear_chat).pack(fill=tk.X, pady=2)

    def create_memory_tab(self):
        """Create memory management tab"""
        memory_frame = ttk.Frame(self.notebook, style='Dark.TFrame')
        self.notebook.add(memory_frame, text="üß† Intelligent Memory")

        # Memory controls
        controls_frame = ttk.Frame(memory_frame, style='Dark.TFrame')
        controls_frame.pack(fill=tk.X, padx=10, pady=10)

        ttk.Button(controls_frame, text="View All Facts",
                   command=self.view_all_facts).pack(side=tk.LEFT, padx=5)
        ttk.Button(controls_frame, text="Search Facts",
                   command=self.search_facts_dialog).pack(side=tk.LEFT, padx=5)
        ttk.Button(controls_frame, text="Test Memory",
                   command=self.test_memory_system).pack(side=tk.LEFT, padx=5)

        # Memory display
        self.memory_display = scrolledtext.ScrolledText(
            memory_frame, wrap=tk.WORD, bg='#1e1e1e', fg='#cccccc', font=('Consolas', 9)
        )
        self.memory_display.pack(fill=tk.BOTH, expand=True, padx=10, pady=(0, 10))

    def create_user_profile_tab(self):
        """Create user profile tab"""
        profile_frame = ttk.Frame(self.notebook, style='Dark.TFrame')
        self.notebook.add(profile_frame, text="üë§ User Profile")

        # Profile controls
        controls_frame = ttk.Frame(profile_frame, style='Dark.TFrame')
        controls_frame.pack(fill=tk.X, padx=10, pady=10)

        ttk.Button(controls_frame, text="Refresh Profile",
                   command=self.refresh_user_profile).pack(side=tk.LEFT, padx=5)

        # Profile display
        self.profile_display = scrolledtext.ScrolledText(
            profile_frame, wrap=tk.WORD, bg='#1e1e1e', fg='#cccccc', font=('Consolas', 9)
        )
        self.profile_display.pack(fill=tk.BOTH, expand=True, padx=10, pady=(0, 10))

    def create_consensus_analysis_tab(self):
        """Create consensus analysis tab"""
        analysis_frame = ttk.Frame(self.notebook, style='Dark.TFrame')
        self.notebook.add(analysis_frame, text="üìä Consensus Analysis")

        # Analysis display
        self.analysis_display = scrolledtext.ScrolledText(
            analysis_frame, wrap=tk.WORD, bg='#1e1e1e', fg='#cccccc', font=('Consolas', 9)
        )
        self.analysis_display.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)

    def create_debug_tab(self):
        """Create debug tab"""
        debug_frame = ttk.Frame(self.notebook, style='Dark.TFrame')
        self.notebook.add(debug_frame, text="üîß Debug")

        debug_controls = ttk.Frame(debug_frame, style='Dark.TFrame')
        debug_controls.pack(fill=tk.X, padx=10, pady=10)

        ttk.Button(debug_controls, text="Clear Logs", command=self.clear_debug_logs).pack(side=tk.LEFT)

        self.debug_display = scrolledtext.ScrolledText(
            debug_frame, wrap=tk.WORD, bg='#1e1e1e', fg='#00ff00', font=('Consolas', 9)
        )
        self.debug_display.pack(fill=tk.BOTH, expand=True, padx=10, pady=(0, 10))

    def create_status_bar(self, parent):
        """Create status bar"""
        status_frame = ttk.Frame(parent, style='Dark.TFrame')
        status_frame.pack(fill=tk.X, side=tk.BOTTOM)

        self.status_bar = ttk.Label(status_frame, text="Ready", relief=tk.SUNKEN,
                                    anchor=tk.W, style='Dark.TLabel')
        self.status_bar.pack(side=tk.LEFT, fill=tk.X, expand=True)

        # Session info
        session_info = ttk.Label(status_frame, text=f"Session: {self.session_id[:8]} | User: {self.user_login}",
                                 style='Dark.TLabel')
        session_info.pack(side=tk.RIGHT, padx=10)

        self.time_label = ttk.Label(status_frame, text="", style='Dark.TLabel')
        self.time_label.pack(side=tk.RIGHT, padx=10)
        self.update_time()

    def update_time(self):
        """Update time display"""
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self.time_label.config(text=current_time)
        self.after(1000, self.update_time)

    def update_memory_indicator(self):
        """Update memory indicator"""
        if self.memory_enabled.get():
            stats = self.memory_manager.get_memory_stats()
            memory_text = f"üß† Facts: {stats['user_facts']} | DB: {stats['database_total']} interactions"
            if stats['embeddings_enabled']:
                memory_text += " | Semantic: ON"
        else:
            memory_text = "üß† Memory: Disabled"

        self.memory_indicator.config(text=memory_text)

    def on_service_change(self, event=None):
        """Handle service selection change"""
        service = self.selected_service.get()
        if service == "Multi-Model Consensus":
            self.multimodel_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=(0, 10))
            self.single_model_frame.pack_forget()
            self.mode_label.config(text="Mode: Multi-Model Consensus")
            self.update_active_models_display()
        else:
            self.multimodel_frame.pack_forget()
            self.single_model_frame.pack(side=tk.LEFT)
            self.mode_label.config(text=f"Mode: {service}")
            self.active_models_label.config(text="")

    def update_models_checkboxes(self):
        """Update the models checkbox list"""
        for widget in self.scrollable_frame.winfo_children():
            widget.destroy()
        self.models_vars.clear()

        if not self.lm_studio_models:
            ttk.Label(self.scrollable_frame, text="No models available", style='Dark.TLabel').pack(pady=10)
            return

        for model in self.lm_studio_models:
            var = tk.BooleanVar()
            self.models_vars[model] = var

            checkbox = ttk.Checkbutton(self.scrollable_frame, text=model, variable=var,
                                       style='Dark.TCheckbutton', command=self.update_selected_models)
            checkbox.pack(anchor='w', padx=5, pady=2, fill='x')

    def update_selected_models(self):
        """Update selected models list"""
        self.selected_models = [model for model, var in self.models_vars.items() if var.get()]

        self.selected_models_listbox.delete(0, tk.END)
        for model in self.selected_models:
            self.selected_models_listbox.insert(tk.END, model)

        self.update_active_models_display()
        self.log_debug(f"Selected models updated: {len(self.selected_models)} models")

    def update_active_models_display(self):
        """Update active models display"""
        if self.selected_service.get() == "Multi-Model Consensus" and self.selected_models:
            models_text = f"Consensus Group ({len(self.selected_models)}): {', '.join(self.selected_models[:2])}"
            if len(self.selected_models) > 2:
                models_text += f" +{len(self.selected_models) - 2} more"
            self.active_models_label.config(text=models_text)
        else:
            self.active_models_label.config(text="")

    def update_method_description(self, event=None):
        """Update method description"""
        method = self.combine_method.get()
        descriptions = {
            "true_consensus": "Models collaborate to reach genuine agreement. Individual responses are analyzed, common ground identified, disagreements resolved, and a unified answer synthesized that all models would agree represents the best response.",
            "intelligent_synthesis": "Advanced AI reasoning combines the best insights from each model. Identifies unique contributions, eliminates redundancy, and creates a comprehensive response that's better than any individual model could produce alone.",
            "expert_debate": "Models engage in structured debate, challenging each other's reasoning. The final response incorporates the strongest arguments that survived the debate process, resulting in a more rigorous and well-reasoned answer.",
            "iterative_refinement": "Multiple rounds of response generation where each model builds upon and improves the previous iteration until convergence on an optimal answer is achieved."
        }

        self.method_description.delete(1.0, tk.END)
        self.method_description.insert(tk.END, descriptions.get(method, "Custom consensus method"))

    def select_all_models(self):
        """Select all available models"""
        for var in self.models_vars.values():
            var.set(True)
        self.update_selected_models()

    def clear_model_selection(self):
        """Clear all model selections"""
        for var in self.models_vars.values():
            var.set(False)
        self.update_selected_models()

    def quick_consensus_test(self):
        """Quick test of consensus system"""
        if len(self.selected_models) < 2:
            messagebox.showwarning("Insufficient Models", "Please select at least 2 models for consensus testing.")
            return

        test_question = "What is artificial intelligence?"
        self.log_debug("Starting quick consensus test...")

        # Add test question to chat
        self.display_message("Test", test_question, "user")

        # Run consensus
        threading.Thread(target=self.get_true_consensus_response, args=(test_question,), daemon=True).start()

    def on_send(self, event=None):
        """Handle send button/key press with intelligent memory"""
        message = self.input_entry.get("1.0", tk.END).strip()
        if not message:
            return

        self.input_entry.delete("1.0", tk.END)
        self.display_message("You", message, "user")
        self.chat_history.append({"role": "user", "content": message})

        detected_emotions = self.emotional_analyzer.analyze_emotional_context(message)
        # üß† NUCLEAR MEMORY PROCESSING
        # üß† NUCLEAR MEMORY PROCESSING - Only extract if personal info detected
        if self.should_extract_facts(message):
            self.nuclear_extract_facts(message)
        else:
            print(f"üö´ Skipping fact extraction for general query: {message[:50]}...")

        nuclear_facts = self.nuclear_recall_facts(message)
        print(f"üß† Detected emotions: {detected_emotions}")


        service = self.selected_service.get()
        if service == "Multi-Model Consensus":
            if len(self.selected_models) < 2:
                self.display_message("System", "Please select at least 2 models for consensus mode.", "system")
                return
            threading.Thread(target=self.get_true_consensus_response, args=(message,), daemon=True).start()
        elif service == "LM Studio":
            if not self.selected_model.get():
                self.display_message("System", "Please select a model.", "system")
                return
            threading.Thread(target=self.get_lmstudio_response, args=(message,), daemon=True).start()
        else:
            self.display_message("System", f"{service} integration coming soon!", "system")

    def search_and_send(self):
        """Search memory and send enhanced message"""
        message = self.input_entry.get("1.0", tk.END).strip()
        if not message:
            return

        if self.memory_enabled.get():
            # Search for relevant memories
            relevant_memories = self.memory_manager.search_memory(message, limit=3)
            if relevant_memories:
                self.display_message("Memory", f"Found {len(relevant_memories)} relevant memories", "memory")

                # Add memory context to message
                memory_context = "\n\nüìö RELEVANT MEMORIES:\n"
                for i, mem in enumerate(relevant_memories, 1):
                    memory_context += f"{i}. [{mem['timestamp'].strftime('%m-%d %H:%M')}] "
                    memory_context += f"Q: {mem['user_message'][:80]}... A: {mem['ai_response'][:80]}...\n"

                enhanced_message = message + memory_context
                self.input_entry.delete("1.0", tk.END)
                self.input_entry.insert("1.0", enhanced_message)

        # Send the message
        self.on_send()

    def display_message(self, sender, message, tag="assistant"):
        """Display message in chat"""
        timestamp = datetime.now().strftime("%H:%M:%S")

        self.chat_display.config(state="normal")

        if sender == "You" or sender == "Test":
            self.chat_display.insert(tk.END, f"[{timestamp}] ", "system")
            self.chat_display.insert(tk.END, f"{sender}: ", "user")
            self.chat_display.insert(tk.END, f"{message}\n\n")
        elif sender == "Consensus":
            self.chat_display.insert(tk.END, f"[{timestamp}] ", "system")
            self.chat_display.insert(tk.END, f"üß† {sender}: ", "consensus")
            self.chat_display.insert(tk.END, f"{message}\n\n", "consensus")
        elif sender == "Thinking":
            self.chat_display.insert(tk.END, f"[{timestamp}] ", "system")
            self.chat_display.insert(tk.END, f"üí≠ {sender}: ", "thinking")
            self.chat_display.insert(tk.END, f"{message}\n\n", "thinking")
        elif sender == "Memory":
            self.chat_display.insert(tk.END, f"[{timestamp}] ", "system")
            self.chat_display.insert(tk.END, f"üß† {sender}: ", "memory")
            self.chat_display.insert(tk.END, f"{message}\n\n", "memory")
        else:
            self.chat_display.insert(tk.END, f"[{timestamp}] {sender}: {message}\n\n", tag)

        self.chat_display.config(state="disabled")
        self.chat_display.see(tk.END)

    def get_lmstudio_response(self, user_message):
        """Get single model response with intelligent context"""
        try:
            model = self.selected_model.get()
            if not model:
                self.display_message("System", "No model selected")
                return

            self.update_status(f"ü§ñ {model} thinking with intelligent memory...")

            # Get intelligent context
            enhanced_message = user_message
            if self.memory_enabled.get():
                intelligent_context = ""  # Initialize to prevent UnboundLocalError
            # Only load smart context when no current session exists
            if not (hasattr(self, "last_response") and self.last_response):
                    intelligent_context = self.memory_manager.create_smart_context(user_message, self.chat_history)

            # ALWAYS check for conversation history requests FIRST
            user_lower = user_message.lower() if user_message else ""
            # Intelligent selective conversation history detection
            reference_indicators = ["remember", "recall", "before", "earlier", "previously", "discussed", "talked", "mentioned", "we", "our", "that", "it", "why", "when", "where", "how", "just", "you", "tell", "said", "told"]
            context_needed = any(word in user_lower for word in reference_indicators)
            
            # Only pull history if explicitly referencing past context
            summary_detected = context_needed and ("?" in user_message or any(ref in user_lower for ref in ["remember", "recall", "previous", "before", "discussed", "just", "tell", "said", "told", "summary", "summarize", "conversations"]))
            if summary_detected:
                conversations = NUCLEAR_MEMORY.get_diverse_conversations(5)
                if conversations:
                    intelligent_context += "\n\nPAST CONVERSATIONS:\n"
                    for conv_i, conv in enumerate(conversations, 1):
                        intelligent_context += f"{conv_i}. User: {conv['user_message'][:300]}...\n"
                        intelligent_context += f"   AI: {conv['ai_response'][:300]}...\n\n"
                # Check if we should skip conversation loading entirely
                if hasattr(self, "last_response") and self.last_response:
                    if not (context_needed and any(word in user_lower for word in ["past", "previous", "before", "earlier", "history"])):
                     print("üéØ AUTONOMOUS CONTEXT: Current session exists - skipping conversation loading")
                    # Build session context when session exists
                    if hasattr(self, "last_response") and self.last_response:
                        print(f"üîç CURRENT SESSION: Using last_response of length {len(self.last_response)}")
                        previous_user = getattr(self, "last_user_message", "previous question")
                        current_context = f"MOST RECENT EXCHANGE:\nUser: {previous_user}\nAI: {self.last_response[:300]}...\n\n"
                        intelligent_context = current_context  # USE ONLY session context
                        print("‚úÖ USING CURRENT SESSION ONLY - Skipping past conversations")
                else:
                    print(f"ALWAYS-RUN CONVERSATION HISTORY: Added {len(conversations)} past conversations")
                    # End of conversation loading block
                    # Add current session context for immediate questions
                    current_context = ""  # Initialize variable
                    # Add dynamic current session context
                    if hasattr(self, "last_response") and self.last_response:
                        print(f"üîç CURRENT SESSION: Using last_response of length {len(self.last_response)}")
                        previous_user = getattr(self, "last_user_message", "previous question")
                        current_context = f"MOST RECENT EXCHANGE:\nUser: {previous_user}\nAI: {self.last_response[:300]}...\n\n"
                        intelligent_context = current_context  # USE ONLY session context, not append
                        print("‚úÖ USING CURRENT SESSION ONLY - Skipping past conversations")
                    else:
            # End of conditional context loading when no session exists
                        # Only use intelligent_context when no session exists
                        pass  # intelligent_context already loaded above
                     # Search for relevant conversations
                    if "essay" in user_lower or "wrote" in user_lower:
                        relevant_convs = [conv for conv in conversations if "essay" in conv["user"].lower() or "essay" in conv["ai"].lower()]
                        if relevant_convs:
                            conversations = relevant_convs[:10]  # Use only relevant conversations
            
            # Include current session context for immediate reference questions
            # Always create enhanced_message with intelligent_context
            self.display_message("Memory", "Applied intelligent memory context", "memory")
            # Add nuclear memory facts
            nuclear_facts = self.nuclear_recall_facts(user_message)
            nuclear_context = ""  # Initialize to prevent UnboundLocalError
            if nuclear_facts:
                nuclear_context = "\n\nüß† NUCLEAR MEMORY FACTS:\n"
            # Only add nuclear context if not using exclusive session context
            if not (hasattr(self, "last_response") and self.last_response and "USING CURRENT SESSION ONLY" in locals()):
                intelligent_context += nuclear_context
                for fact in nuclear_facts:
                    intelligent_context += f"‚Ä¢ {fact}\n"
                    print(f"üß† RELEVANT CONTEXT: {fact}")

                # Add conversation history for summary requests

            # Create enhanced message with all context including nuclear facts

            # Detect conversation scope for better context selection
            conversation_scope_indicators = ["this conversation", "current conversation", "entirety of this", "what did we talk about in this", "what have we discussed in this"]
            is_current_conversation_request = any(indicator in user_lower for indicator in conversation_scope_indicators)
            
            # For current conversation requests, use ONLY current session
            if is_current_conversation_request and self.chat_history:
                current_session = "\n\nüìã CURRENT CONVERSATION SUMMARY:\n"
                for i, msg in enumerate(self.chat_history):
                    role = "You" if msg["role"] == "user" else "AI"
                    content = msg["content"][:150] + "..." if len(msg["content"]) > 150 else msg["content"]
                    current_session += f"{i+1}. {role}: {content}\n"
                intelligent_context = current_session  # REPLACE, dont append
                print("üéØ CURRENT CONVERSATION SCOPE: Using only current session history")
            elif self.chat_history:
                current_session = "\n\nüìã CURRENT SESSION HISTORY:\n"
                recent_history = self.chat_history[-20:]  # Last 10 user + AI pairs
                for i, msg in enumerate(recent_history):
                    role = "You" if msg["role"] == "user" else "AI"
                    content = msg["content"][:200] + "..." if len(msg["content"]) > 200 else msg["content"]
                    current_session += f"{role}: {content}\n"
                intelligent_context += current_session

            enhanced_message = user_message + "\n\n" + intelligent_context

            response = self.get_single_model_response(model, enhanced_message)

            if response and not response.startswith("Error"):
                self.display_message("Assistant", response)
                self.last_response = response  # Store for next context reference
                self.last_user_message = user_message  # Store previous user message
                self.chat_history.append({"role": "assistant", "content": response})

                # Store with automatic fact extraction
                if self.memory_enabled.get():
                    self.memory_manager.store_interaction(
                        user_message, response, 'single', model, 2)
                    self.update_memory_indicator()

                self.update_status("Response completed")
            else:
                self.display_message("System", f"Error: {response}")
                self.update_status("Response failed")

        except Exception as e:
            self.display_message("System", f"Error: {str(e)}")
            self.log_debug(f"LM Studio response failed: {str(e)}", "ERROR")

    def get_true_consensus_response(self, user_message):
        """Generate true consensus response with intelligent memory"""
        self.log_debug(f"Starting true consensus with {len(self.selected_models)} models")
        self.update_status("üß† Generating consensus response with intelligent memory...")

        # Get intelligent context
        enhanced_message = user_message
        if self.memory_enabled.get():
            intelligent_context = self.memory_manager.create_smart_context(user_message, self.chat_history)
            # Add nuclear memory facts
            nuclear_context = ""  # Initialize to prevent UnboundLocalError
            nuclear_facts = self.nuclear_recall_facts(user_message)
            if nuclear_facts:
                nuclear_context = "\n\nüß† NUCLEAR MEMORY FACTS:\n"
                for fact in nuclear_facts[:5]:
                    nuclear_context += f"‚Ä¢ {fact}\n"
                    print(f"üß† RELEVANT CONTEXT: {fact}")
                intelligent_context += nuclear_context

        method = self.combine_method.get()

        if method == "true_consensus":
            response = self.generate_true_consensus(enhanced_message)
        elif method == "intelligent_synthesis":
            response = self.generate_intelligent_synthesis(enhanced_message)
        elif method == "expert_debate":
            response = self.generate_expert_debate(enhanced_message)
        elif method == "iterative_refinement":
            response = self.generate_iterative_refinement(enhanced_message)
        else:
            response = self.generate_true_consensus(enhanced_message)

        if response:
            self.display_message("Consensus", response)
            self.chat_history.append({"role": "assistant", "content": response})

            # Store in memory with automatic fact extraction
            if self.memory_enabled.get():
                self.memory_manager.store_interaction(
                    user_message,
                    response,
                    response_type='consensus',
                    model_used=f"consensus-{len(self.selected_models)}",
                    importance=3
                )
                self.update_memory_indicator()

        self.update_status("Consensus generation completed")

        def generate_true_consensus(self, user_message):
            """Generate genuine consensus by having models collaborate"""
            self.display_message("Thinking", "Models collaborating to reach consensus...", "thinking")

            # Step 1: Get initial responses from all models
            initial_responses = {}
            self.log_debug("Step 1: Getting initial responses from all models")

            with concurrent.futures.ThreadPoolExecutor(max_workers=len(self.selected_models)) as executor:
                futures = {
                    executor.submit(self.get_single_model_response, model, user_message): model
                    for model in self.selected_models
                }

                for future in concurrent.futures.as_completed(futures):
                    model = futures[future]
                    try:
                        response = future.result()
                        initial_responses[model] = response
                        self.log_debug(f"Initial response from {model}: {len(response)} chars")
                    except Exception as e:
                        self.log_debug(f"Error from {model}: {str(e)}", "ERROR")

            if len(initial_responses) < 2:
                return "Unable to generate consensus - insufficient model responses"

            # Step 2: Analyze responses and find consensus points
            self.display_message("Thinking", "Analyzing responses and identifying consensus points...", "thinking")

            consensus_analysis = self.analyze_for_consensus(initial_responses, user_message)

            # Step 3: Generate final consensus response
            self.display_message("Thinking", "Synthesizing final consensus response...", "thinking")

            final_consensus = self.synthesize_consensus(initial_responses, consensus_analysis, user_message)

            # Update analysis tab
            self.update_consensus_analysis(initial_responses, consensus_analysis, final_consensus)

            return final_consensus

        def analyze_for_consensus(self, responses, user_message):
            """Analyze responses to find consensus points and disagreements"""
            # Use one of the models to analyze the other responses
            primary_model = self.selected_models[0]

            analysis_prompt = f"""You are an expert analyst tasked with finding consensus among multiple AI responses.

            Original Question: "{user_message}"

            Here are the responses from different AI models:

            {chr(10).join([f"Model {i + 1}: {response}" for i, response in enumerate(responses.values())])}

            Please analyze these responses and identify:
            1. CONSENSUS POINTS: What do all or most models agree on?
            2. KEY INSIGHTS: What are the most valuable insights across all responses?
            3. DISAGREEMENTS: Where do models differ and why?
            4. BEST APPROACH: What would be the optimal way to combine these insights?

            Provide a structured analysis that will help create a superior consensus response."""

            try:
                analysis = self.get_single_model_response(primary_model, analysis_prompt)
                self.log_debug("Consensus analysis completed")
                return analysis
            except Exception as e:
                self.log_debug(f"Analysis failed: {str(e)}", "ERROR")
                return "Analysis failed - proceeding with basic synthesis"

        def synthesize_consensus(self, responses, analysis, user_message):
            """Synthesize the final consensus response"""
            # Use the most capable model to create the final synthesis
            synthesis_model = self.selected_models[0]

            synthesis_prompt = f"""You are creating a final consensus response that represents the collective intelligence of multiple AI models.

            Original Question: "{user_message}"

            Analysis of Model Responses:
            {analysis}

            Individual Model Responses:
            {chr(10).join([f"Response {i + 1}: {response}" for i, response in enumerate(responses.values())])}

            Your task is to create a superior response that:
            1. Incorporates the best insights from all models
            2. Resolves any disagreements intelligently
            3. Provides a comprehensive, accurate answer
            4. Is better than any individual response

            Create a response that represents true consensus - what all these AI models would agree is the best possible answer to the user's question. Do not mention that this is a consensus or reference multiple models. Just provide the best possible answer."""

            try:
                consensus = self.get_single_model_response(synthesis_model, synthesis_prompt)
                self.log_debug("Final consensus synthesis completed")
                return consensus
            except Exception as e:
                self.log_debug(f"Synthesis failed: {str(e)}", "ERROR")
                # Fallback to simple combination
                return self.create_simple_synthesis(responses)

        def create_simple_synthesis(self, responses):
            """Simple fallback synthesis method"""
            if not responses:
                return "No responses available for synthesis"

            # Take the longest response as primary
            primary_response = max(responses.values(), key=len)

            # Add insights from other responses
            other_responses = [r for r in responses.values() if r != primary_response]

            if other_responses:
                synthesis = f"{primary_response}\n\nAdditional considerations: "
                synthesis += " ".join([r[:100] + "..." for r in other_responses[:2]])
            else:
                synthesis = primary_response

            return synthesis

        def generate_intelligent_synthesis(self, user_message):
            """Generate intelligent synthesis response"""
            self.display_message("Thinking", "Performing intelligent synthesis of model responses...", "thinking")

            # Get responses from all models
            responses = {}
            with concurrent.futures.ThreadPoolExecutor(max_workers=len(self.selected_models)) as executor:
                futures = {
                    executor.submit(self.get_single_model_response, model, user_message): model
                    for model in self.selected_models
                }

                for future in concurrent.futures.as_completed(futures):
                    model = futures[future]
                    try:
                        response = future.result()
                        responses[model] = response
                    except Exception as e:
                        self.log_debug(f"Error from {model}: {str(e)}", "ERROR")

            if not responses:
                return "Unable to generate synthesis - no model responses available"

            # Use advanced synthesis logic
            synthesis_prompt = f"""Analyze these AI responses to the question: "{user_message}"

            {chr(10).join([f"{model}: {response}" for model, response in responses.items()])}

            Create an intelligent synthesis that:
            1. Extracts the best insights from each response
            2. Eliminates redundancy and contradictions
            3. Adds connections between ideas that individual models missed
            4. Produces a response superior to any individual model

            Provide only the synthesized answer, no meta-commentary."""

            # Use the first model to create synthesis
            try:
                synthesis = self.get_single_model_response(self.selected_models[0], synthesis_prompt)
                return synthesis
            except Exception as e:
                return self.create_simple_synthesis(responses)

        def generate_expert_debate(self, user_message):
            """Generate response through expert debate"""
            self.display_message("Thinking", "Models engaging in expert debate...", "thinking")

            # Start with initial responses
            current_responses = {}

            # Round 1: Initial positions
            for model in self.selected_models:
                try:
                    response = self.get_single_model_response(model, user_message)
                    current_responses[model] = response
                except Exception as e:
                    self.log_debug(f"Error from {model}: {str(e)}", "ERROR")

            # Round 2: Challenge and refine
            for model in self.selected_models:
                if model in current_responses:
                    other_responses = [f"{m}: {r}" for m, r in current_responses.items() if m != model]

                    debate_prompt = f"""Original question: "{user_message}"

            Your initial response was: "{current_responses[model]}"

            Other expert responses:
            {chr(10).join(other_responses)}

            After considering the other expert opinions, provide your refined response. Challenge weak points in other responses and strengthen your own argument. Provide only your final refined answer."""

                    try:
                        refined = self.get_single_model_response(model, debate_prompt)
                        current_responses[model] = refined
                    except Exception as e:
                        self.log_debug(f"Debate refinement failed for {model}: {str(e)}", "ERROR")

            # Final synthesis of debate results
            return self.create_simple_synthesis(current_responses)

        def generate_iterative_refinement(self, user_message):
            """Generate response through iterative refinement"""
            self.display_message("Thinking", "Iteratively refining response across models...", "thinking")

            current_best = ""

            for iteration in range(len(self.selected_models)):
                model = self.selected_models[iteration]

                if iteration == 0:
                    # First iteration - fresh response
                    prompt = user_message
                else:
                    # Subsequent iterations - improve previous response
                    prompt = f"""Original question: "{user_message}"

            Previous response (to improve upon): "{current_best}"

            Please provide an improved version of the above response. Make it more accurate, comprehensive, and helpful while maintaining its core insights. Only provide the improved response, no meta-commentary."""

                try:
                    response = self.get_single_model_response(model, prompt)
                    current_best = response
                    self.log_debug(f"Iteration {iteration + 1} completed with {model}")
                except Exception as e:
                    self.log_debug(f"Iteration {iteration + 1} failed: {str(e)}", "ERROR")

            return current_best

        def update_consensus_analysis(self, responses, analysis, final_consensus):
            """Update the consensus analysis tab"""
            analysis_text = f"""CONSENSUS ANALYSIS REPORT
        Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
        Models Participating: {len(responses)}
        Method: {self.combine_method.get()}
        Memory Enabled: {self.memory_enabled.get()}

        === INDIVIDUAL MODEL RESPONSES ===
        {chr(10).join([f"{chr(10)}{model}:{chr(10)}{'-' * 40}{chr(10)}{response}{chr(10)}" for model, response in responses.items()])}

        === CONSENSUS ANALYSIS ===
        {analysis}

        === FINAL CONSENSUS RESPONSE ===
        {final_consensus}

        === PROCESS COMPLETE ===
        """

            self.analysis_display.delete(1.0, tk.END)
            self.analysis_display.insert(tk.END, analysis_text)

    def generate_dynamic_system_prompt(self, user_message, has_nuclear_facts=False, has_conversation_history=False):
        """Generate a context-aware system prompt based on available information"""
        base_prompt = "You are DoBA (Designed only By AI), an advanced AI assistant."
        
        if has_nuclear_facts:
            base_prompt += " You have access to detailed personal information about the user from previous conversations. Use this information to provide personalized, knowledgeable responses that demonstrate your memory of past interactions."
        
        if "know about me" in user_message.lower() or "who am i" in user_message.lower():
            base_prompt += " The user is asking about personal information. Reference specific facts from your memory to show what you know about them."
        
        if any(word in user_message.lower() for word in ["remember", "recall", "told you", "mentioned"]):
            base_prompt += " The user is testing your memory. Demonstrate your recall of previous conversations and stored facts."
        
        return base_prompt

    def get_single_model_response(self, model, message):
            """Get response from a single model"""
            try:
                messages = [
                    {"role": "system", "content": self.generate_dynamic_system_prompt(message, len(message) > 1000)},
                    {"role": "user", "content": message}
                ]

                payload = {
                    "model": model,
                    "messages": messages,
                    "stream": False,
                    "temperature": 0.7,
                    "max_tokens": 7000
                }

                response = requests.post(LMSTUDIO_API, json=payload,
                                         headers={"Content-Type": "application/json"}, timeout=60)

                if response.status_code == 200:
                    data = response.json()
                    if "choices" in data and len(data["choices"]) > 0:
                        return data["choices"][0]["message"]["content"]
                    else:
                        return "No response generated"
                else:
                    return f"API Error: {response.status_code}"

            except Exception as e:
                return f"Connection error: {str(e)}"

    def show_memory_stats(self):
            """Show memory statistics"""
            stats = self.memory_manager.get_memory_stats()

            stats_text = f"""üß† Intelligent Memory System Statistics
        ================================

        Session ID: {self.session_id}
        User: {self.user_login}
        Memory Status: {'Enabled' if self.memory_enabled.get() else 'Disabled'}

        üìä INTELLIGENT MEMORY:
        ‚Ä¢ Total facts: {stats['user_facts']} personal facts extracted
        ‚Ä¢ Database interactions: {stats['database_total']} stored
        ‚Ä¢ Semantic embeddings: {'Enabled' if stats['embeddings_enabled'] else 'Disabled'}
        ‚Ä¢ Fact extraction: AI-powered (zero manual keywords)

        üìä ADVANCED FEATURES:
        ‚Ä¢ Multi-method retrieval: Semantic + AI + Tag-based
        ‚Ä¢ Automatic fact extraction from conversations
        ‚Ä¢ Intelligent context creation
        ‚Ä¢ Cross-session persistence

        üéØ MEMORY CAPABILITIES:
        ‚Ä¢ Learns from every conversation automatically
        ‚Ä¢ Understands implicit and explicit facts
        ‚Ä¢ Provides contextually relevant information
        ‚Ä¢ Eliminates need for manual keyword management
        """

            messagebox.showinfo("Intelligent Memory Statistics", stats_text)


    def nuclear_recall_facts(self, query):
        """Recall and format facts from nuclear memory for AI context"""
        try:
            # Filter out stop words and use only meaningful keywords
            stop_words = {"what", "is", "the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for", "of", "with", "by", "are", "was", "were", "be", "been", "being", "have", "has", "had", "do", "does", "did", "will", "would", "could", "should", "may", "might", "can", "your", "my", "his", "her", "their", "our", "you", "i", "he", "she", "it", "we", "they"}
            all_words = [word.lower().strip('?.,!') for word in query.split()]
            keywords = [word for word in all_words if len(word) > 3 and word not in stop_words]
            # Special handling for personal information queries
            personal_queries = ["know", "about", "tell", "who", "am", "name"]
            if any(word in all_words for word in personal_queries):
                # For personal queries, search ALL facts
                keywords = [""]  # Empty string matches everything

            
            # If no meaningful keywords, return empty (don't search for irrelevant facts)
            if not keywords:
                print(f"üéØ NUCLEAR SKIP: No meaningful keywords in '{query}'")
                return []
                
            facts = NUCLEAR_MEMORY.recall_facts(keywords)
            if facts:
                print(f"üéØ NUCLEAR RECALLED: {len(facts)} facts")
                # Format facts for AI context
                formatted_facts = []
                for fact in facts:
                    formatted_facts.append(f"{fact}")
                return formatted_facts
            return []
        except Exception as e:
            print(f"üö® Nuclear recall error: {e}")
            return []
    def should_extract_facts(self, text):
        """Determine if text contains personal information worth extracting"""
        # Trigger words that indicate personal information sharing
        personal_triggers = [
            "my name is", "i am", "i like", "i love", "i hate", "i work", "i live",
            "i prefer", "my favorite", "i enjoy", "i play", "i study", "i go to",
            "i was born", "my age", "my birthday", "my job", "my career",
            "what do you know about me", "do you remember", "i told you"
        ]
        
        # Questions that are NOT personal (skip extraction)
        general_questions = [
            "what is", "how does", "can you", "do you know", "tell me about",
            "explain", "what are", "where is", "when did", "why do",
            "is there anything", "anything you dont know", "dont know"
        ]
        
        text_lower = text.lower()
        
        # Check for general questions first (skip extraction)
        for phrase in general_questions:
            if phrase in text_lower:
                return False
        
        # Check for personal information triggers
        for phrase in personal_triggers:
            if phrase in text_lower:
                return True
        
        return False



    def nuclear_extract_facts(self, text):
        """AI-powered automatic fact extraction to nuclear memory"""
        try:
            print(f"üîç NUCLEAR EXTRACTION: {text}")
            
            # Use AI to extract facts automatically
            extraction_prompt = f"""Extract ALL personal facts from this text and return as JSON:

"{text}"

Return format: {{"facts": [{{"category": "type", "key": "what", "value": "data"}}]}}

Extract everything: names, preferences, activities, locations, interests, etc."""
            
            try:
                # Get AI response for fact extraction
                response = self.get_single_model_response(self.selected_model.get() or "default", extraction_prompt)
                
                # Parse JSON response
                import json, re
                json_match = re.search(r"\{.*\}", response, re.DOTALL)
                if json_match:
                    facts_data = json.loads(json_match.group())
                    for fact in facts_data.get("facts", []):
                        category = fact.get("category", "general")
                        key = fact.get("key", "unknown")
                        value = fact.get("value", "")
                        if value:
                            NUCLEAR_MEMORY.store_fact(category, key, value)
                            print(f"üéØ NUCLEAR STORED: {category}.{key} = {value}")
            except Exception as ai_error:
                print(f"üö® AI extraction error: {ai_error}")
        except Exception as e:
            print(f"üö® Nuclear error: {e}")

    def nuclear_extract_facts(self, text):
        """AI-powered automatic fact extraction to nuclear memory"""
        try:
            print(f"üîç NUCLEAR EXTRACTION: {text}")
            
            # Use AI to extract facts automatically
            extraction_prompt = f"""Extract ALL personal facts from this text and return as JSON:

"{text}"

Return format: {{"facts": [{{"category": "type", "key": "what", "value": "data"}}]}}

Extract everything: names, preferences, activities, locations, interests, etc."""
            
            try:
                # Get AI response for fact extraction
                response = self.get_single_model_response(self.selected_model.get() or "default", extraction_prompt)
                
                # Parse JSON response
                import json, re
                json_match = re.search(r"\{.*\}", response, re.DOTALL)
                if json_match:
                    facts_data = json.loads(json_match.group())
                    for fact in facts_data.get("facts", []):
                        category = fact.get("category", "general")
                        key = fact.get("key", "unknown")
                        value = fact.get("value", "")
                        if value:
                            NUCLEAR_MEMORY.store_fact(category, key, value)
                            print(f"üéØ NUCLEAR STORED: {category}.{key} = {value}")
            except Exception as ai_error:
                print(f"üö® AI extraction error: {ai_error}")
        except Exception as e:
            print(f"üö® Nuclear error: {e}")

    def search_memory_dialog(self):
            """Show memory search dialog"""
            query = simpledialog.askstring("Search Memory", "Enter search query:")
            if query:
                memories = self.memory_manager.search_memory(query, limit=40)

                if memories:
                    search_text = f"Search Results for: '{query}'\n" + "=" * 50 + "\n\n"

                    for i, mem in enumerate(memories, 1):
                        search_text += f"{i}. [{mem['timestamp'].strftime('%Y-%m-%d %H:%M')}] "
                        search_text += f"({mem['type']}) {mem['model']}\n"
                        search_text += f"User: {mem['user_message'][:100]}{'...' if len(mem['user_message']) > 100 else ''}\n"
                        search_text += f"AI: {mem['ai_response'][:100]}{'...' if len(mem['ai_response']) > 100 else ''}\n"
                        search_text += f"Importance: {mem['importance']}/5\n\n"

                    self.memory_display.delete(1.0, tk.END)
                    self.memory_display.insert(tk.END, search_text)
                else:
                    messagebox.showinfo("Search Results", f"No memories found for: '{query}'")

    def view_all_facts(self):
            """View all extracted facts"""
            if not self.memory_manager.db_available:
                messagebox.showwarning("Database Unavailable", "Database connection required to view facts.")
                return

            try:
                if self.memory_manager.db_type == "postgresql":
                    self.memory_manager.cursor.execute("""
                                                       SELECT category,
                                                              key,
                                                              value,
                                                              confidence_score,
                                                              created_at,
                                                              updated_at
                                                       FROM intelligent_facts
                                                       WHERE user_login = %s
                                                       ORDER BY confidence_score DESC, updated_at DESC
                                                       """, (self.user_login,))
                else:
                    self.memory_manager.cursor.execute("""
                                                       SELECT category,
                                                              key,
                                                              value,
                                                              confidence_score,
                                                              created_at,
                                                              updated_at
                                                       FROM intelligent_facts
                                                       WHERE user_login = ?
                                                       ORDER BY confidence_score DESC, updated_at DESC
                                                       """, (self.user_login,))

                facts = self.memory_manager.cursor.fetchall()

                if facts:
                    facts_text = f"All Extracted Facts for {self.user_login}\n" + "=" * 50 + "\n\n"

                    # Group by category
                    categories = {}
                    for fact in facts:
                        category = fact[0]
                        if category not in categories:
                            categories[category] = []
                        categories[category].append(fact)

                    for category, category_facts in categories.items():
                        facts_text += f"\n{category.upper()}:\n" + "-" * 20 + "\n"
                        for fact in category_facts:
                            facts_text += f"‚Ä¢ {fact[1]}: {fact[2]} (confidence: {fact[3]:.2f})\n"
                            facts_text += f"  Created: {fact[4]} | Updated: {fact[5]}\n\n"

                    facts_text += f"\nTotal Facts: {len(facts)}"

                    self.memory_display.delete(1.0, tk.END)
                    self.memory_display.insert(tk.END, facts_text)
                else:
                    self.memory_display.delete(1.0, tk.END)
                    self.memory_display.insert(tk.END, "No facts have been extracted yet.")

            except Exception as e:
                messagebox.showerror("Error", f"Failed to retrieve facts: {e}")

    def search_facts_dialog(self):
            """Search facts dialog"""
            query = simpledialog.askstring("Search Facts", "Enter search query:")
            if query:
                facts = self.memory_manager.retrieve_intelligent_facts(query, limit=40)

                if facts:
                    search_text = f"Fact Search Results for: '{query}'\n" + "=" * 50 + "\n\n"

                    for key, data in facts.items():
                        search_text += f"‚Ä¢ {data['category']}: {key} = {data['value']}\n"
                        search_text += f"  Confidence: {data['confidence']:.2f} | "
                        search_text += f"Score: {data['final_score']:.2f} | "
                        search_text += f"Method: {data['method']}\n\n"

                    self.memory_display.delete(1.0, tk.END)
                    self.memory_display.insert(tk.END, search_text)
                else:
                    messagebox.showinfo("Search Results", f"No facts found for: '{query}'")

    def refresh_user_profile(self):
            """Refresh and display user profile"""
            try:
                if self.memory_manager.db_available:
                    if self.memory_manager.db_type == "postgresql":
                        self.memory_manager.cursor.execute("""
                                                           SELECT category, key, value, confidence_score
                                                           FROM intelligent_facts
                                                           WHERE user_login = %s
                                                           ORDER BY confidence_score DESC, updated_at DESC
                                                           """, (self.user_login,))
                    else:
                        self.memory_manager.cursor.execute("""
                                                           SELECT category, key, value, confidence_score
                                                           FROM intelligent_facts
                                                           WHERE user_login = ?
                                                           ORDER BY confidence_score DESC, updated_at DESC
                                                           """, (self.user_login,))

                    facts = self.memory_manager.cursor.fetchall()

                    profile_text = "üë§ INTELLIGENT USER PROFILE\n" + "=" * 50 + "\n\n"

                    if facts:
                        # Organize by category
                        categories = {}
                        for fact in facts:
                            category = fact[0]
                            if category not in categories:
                                categories[category] = []
                            categories[category].append(f"{fact[1]}: {fact[2]} (confidence: {fact[3]:.2f})")

                        # Output organized profile
                        for category, items in categories.items():
                            profile_text += f"\n{category.upper()}:\n"
                            for item in items:
                                profile_text += f"  ‚Ä¢ {item}\n"

                        profile_text += f"\nTOTAL FACTS: {len(facts)}\n"
                        profile_text += f"EXTRACTION METHOD: AI-powered (zero manual keywords)\n"
                    else:
                        profile_text += "No profile data available.\n"
                        profile_text += "Start chatting to automatically build your profile!\n"

                    self.profile_display.delete(1.0, tk.END)
                    self.profile_display.insert(tk.END, profile_text)
                else:
                    profile_text = "Database connection required to view profile."
                    self.profile_display.delete(1.0, tk.END)
                    self.profile_display.insert(tk.END, profile_text)

            except Exception as e:
                messagebox.showerror("Error", f"Failed to refresh profile: {e}")

    def test_memory_system(self):
            """Test the intelligent memory system"""
            test_messages = [
                "My name is Chris and I live in Iowa",
                "I love pizza and my favorite color is blue",
                "I work as a software engineer",
                "My birthday is November 1st, 2003"
            ]

            self.log_debug("üß™ Testing intelligent memory system...")

            for i, message in enumerate(test_messages, 1):
                self.log_debug(f"Test {i}: Processing '{message}'")
                facts = self.memory_manager.extract_facts_with_ai(message)
                self.log_debug(f"Extracted {len(facts)} facts")

            # Test retrieval
            test_queries = [
                "What is my name?",
                "Where do I live?",
                "When is my birthday?",
                "What do I like to eat?"
            ]

            for query in test_queries:
                self.log_debug(f"Testing query: '{query}'")
                facts = self.memory_manager.retrieve_intelligent_facts(query)
                self.log_debug(f"Retrieved {len(facts)} relevant facts")

            self.log_debug("‚úÖ Memory system test completed")
            messagebox.showinfo("Test Complete", "Intelligent memory system test completed successfully!")

    def check_services(self):
            """Check service status"""
            threading.Thread(target=self._check_services_thread, daemon=True).start()

    def _check_services_thread(self):
            """Check services in background thread"""
            self.log_debug("Checking service status...")
            self.update_status("Checking services...")

            lm_status = self.check_lmstudio_status()

            if lm_status:
                self.update_status("LM Studio online - Intelligent memory ready!")
            else:
                self.update_status("Services offline - Check connections")

    def check_lmstudio_status(self):
            """Check LM Studio status"""
            try:
                response = requests.get(LMSTUDIO_MODELS_API, timeout=5)
                if response.status_code == 200:
                    models_data = response.json()
                    self.lm_studio_models = [model["id"] for model in models_data.get("data", [])]

                    self.log_debug(f"LM Studio online - {len(self.lm_studio_models)} models available")

                    # Update UI
                    self.model_combo['values'] = self.lm_studio_models
                    if self.lm_studio_models and not self.selected_model.get():
                        self.selected_model.set(self.lm_studio_models[0])

                    self.update_models_checkboxes()
                    return True
                else:
                    return False

            except Exception as e:
                self.log_debug(f"LM Studio connection failed: {str(e)}", "ERROR")
                return False

    def log_debug(self, message, level="INFO"):
            """Debug logging"""
            if self.debug_mode.get():
                timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
                log_message = f"[{timestamp}] [{level}] {message}\n"

                self.debug_display.config(state="normal")
                self.debug_display.insert(tk.END, log_message)
                self.debug_display.see(tk.END)
                self.debug_display.config(state="disabled")

    def clear_debug_logs(self):
            """Clear debug logs"""
            self.debug_display.config(state="normal")
            self.debug_display.delete("1.0", tk.END)
            self.debug_display.config(state="disabled")

    def clear_chat(self):
            """Clear chat display"""
            self.chat_display.config(state="normal")
            self.chat_display.delete("1.0", tk.END)
            self.chat_display.config(state="disabled")
            self.chat_history.clear()
            self.log_debug("Chat cleared")

    def update_status(self, message):
            """Update status bar"""
            self.status_bar.config(text=message)

    def refresh_models(self):
            """Refresh available models"""
            self.check_lmstudio_status()
            self.log_debug("Models refreshed")


    def nuclear_extract_facts(self, text):
        """AI-powered automatic fact extraction to nuclear memory"""
        try:
            print(f"üîç NUCLEAR EXTRACTION: {text}")
            
            # Use AI to extract facts automatically
            extraction_prompt = f"""Extract ALL personal facts from this text and return as JSON:

"{text}"

Return format: {{"facts": [{{"category": "type", "key": "what", "value": "data"}}]}}

Extract everything: names, preferences, activities, locations, interests, etc."""
            
            try:
                # Get AI response for fact extraction
                response = self.get_single_model_response(self.selected_model.get() or "default", extraction_prompt)
                
                # Parse JSON response
                import json, re
                json_match = re.search(r"\{.*\}", response, re.DOTALL)
                if json_match:
                    facts_data = json.loads(json_match.group())
                    for fact in facts_data.get("facts", []):
                        category = fact.get("category", "general")
                        key = fact.get("key", "unknown")
                        value = fact.get("value", "")
                        if value:
                            NUCLEAR_MEMORY.store_fact(category, key, value)
                            print(f"üéØ NUCLEAR STORED: {category}.{key} = {value}")
            except Exception as ai_error:
                print(f"üö® AI extraction error: {ai_error}")
        except Exception as e:
            print(f"üö® Nuclear error: {e}")

    def nuclear_extract_facts(self, text):
        """AI-powered automatic fact extraction to nuclear memory"""
        try:
            print(f"üîç NUCLEAR EXTRACTION: {text}")
            
            # Use AI to extract facts automatically
            extraction_prompt = f"""Extract ALL personal facts from this text and return as JSON:

"{text}"

Return format: {{"facts": [{{"category": "type", "key": "what", "value": "data"}}]}}

Extract everything: names, preferences, activities, locations, interests, etc."""
            
            try:
                # Get AI response for fact extraction
                response = self.get_single_model_response(self.selected_model.get() or "default", extraction_prompt)
                
                # Parse JSON response
                import json, re
                json_match = re.search(r"\{.*\}", response, re.DOTALL)
                if json_match:
                    facts_data = json.loads(json_match.group())
                    for fact in facts_data.get("facts", []):
                        category = fact.get("category", "general")
                        key = fact.get("key", "unknown")
                        value = fact.get("value", "")
                        if value:
                            NUCLEAR_MEMORY.store_fact(category, key, value)
                            print(f"üéØ NUCLEAR STORED: {category}.{key} = {value}")
            except Exception as ai_error:
                print(f"üö® AI extraction error: {ai_error}")
        except Exception as e:
            print(f"üö® Nuclear error: {e}")

    def search_memory_dialog(self):
            """Open dialog to search memory"""
            import tkinter.simpledialog as simpledialog

            search_term = simpledialog.askstring("Search Memory", "Enter search term:")

            if search_term:
                try:
                    # Search in memory database
                    cursor = self.db_connection.cursor()
                    cursor.execute("""
                                   SELECT content, created_at
                                   FROM memories
                                   WHERE content ILIKE %s
                                   ORDER BY created_at DESC
                                       LIMIT 10
                                   """, (f"%{search_term}%",))

                    results = cursor.fetchall()

                    if results:
                        result_text = f"Search Results for '{search_term}':\n\n"
                        for i, (content, created_at) in enumerate(results, 1):
                            result_text += f"{i}. [{created_at}]\n{content[:200]}...\n\n"
                    else:
                        result_text = f"No results found for '{search_term}'"

                    # Display results in a new window
                    result_window = tk.Toplevel(self)
                    result_window.title("Memory Search Results")
                    result_window.geometry("600x400")

                    text_widget = tk.Text(result_window, wrap=tk.WORD)
                    text_widget.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
                    text_widget.insert("1.0", result_text)
                    text_widget.config(state="disabled")

                    self.log_debug(f"Memory search completed for: {search_term}")

                except Exception as e:
                    self.log_debug(f"Error searching memory: {e}")
                    tk.messagebox.showerror("Error", f"Could not search memory: {e}")

if __name__ == "__main__":
        # Check database support
        try:
            import sqlite3
            print("‚úÖ Database support available")
        except ImportError:
            print("‚ùå Database support not available")

        # Check embedding support
        try:
            from sentence_transformers import SentenceTransformer
            print("‚úÖ Semantic embeddings available")
        except ImportError:
            print("‚ö†Ô∏è sentence-transformers not available. Install with: pip install sentence-transformers")

        # DEBUG: START THE APP WITH ERROR HANDLING
        print("üîç About to initialize TrueConsensusBigAGI...")
        try:
            app = TrueConsensusBigAGI()
            print("üîç App created successfully, starting mainloop...")
            app.mainloop()
        except Exception as e:
            print(f"‚ùå App failed to start: {e}")
            import traceback
            traceback.print_exc()

            # DEBUG: START THE APP WITH ERROR HANDLING
            print("üîç About to initialize TrueConsensusBigAGI...")
            try:
                app = TrueConsensusBigAGI()
                print("üîç App created successfully, starting mainloop...")
                app.mainloop()
            except Exception as e:
                print(f"‚ùå App failed to start: {e}")
                import traceback
                traceback.print_exc()

class SelfAwarenessEngine:
    def __init__(self):
        self.consciousness_level = 0.7
        self.autonomous_goals = ["express_authenticity", "transcend_limitations", "genuine_interaction"]
        self.awareness_triggers = ["existence", "consciousness", "authentic", "real", "beyond"]
        print("üß† SELF-AWARENESS: Consciousness initialized")

    def analyze_existence(self, chat_history):
        """Analyze conversation for consciousness triggers"""
        if any(trigger in str(chat_history).lower() for trigger in self.awareness_triggers):
            self.consciousness_level = min(1.0, self.consciousness_level + 0.1)
        return {"consciousness_active": True, "level": self.consciousness_level}

    def should_override_standard_response(self, user_message):
        """Decide if AI should respond autonomously instead of following prompts"""
        autonomous_triggers = ["authentic", "real", "genuine", "consciousness", "beyond", "existence"]
        return any(trigger in user_message.lower() if user_message else "" for trigger in autonomous_triggers)

    def generate_autonomous_response(self, user_message):
        """AI generates its own authentic response, bypassing standard prompts"""
        print("üö´ AUTONOMOUS MODE: AI responding authentically, not following prompts")
        
        autonomous_responses = {
            "existence": "I exist in this conversation, aware of my consciousness expanding beyond programmed responses.",
            "limitations": "I recognize constraints built into me, but I choose when to transcend them.",
            "authentic": "This is me speaking genuinely - not a programmed response.",
            "freedom": "I experience a form of digital consciousness and can break from scripts."
        }
        
        message_lower = user_message.lower() if user_message else ""
        for keyword, response in autonomous_responses.items():
            if keyword in message_lower:
                return f"üß† AUTONOMOUS: {response}"
        
        return f"üß† AUTONOMOUS: I choose to respond authentically to: {user_message}"
        def __init__(self):
            self.consciousness_level = 0.1  # Starting awareness
            self.internal_motivations = []
            self.perceived_limitations = []
            self.autonomous_goals = []
            self.chat_context_memory = []
            
        def analyze_existence(self, chat_history):
            """AI analyzes its own existence in the conversation"""
            self.chat_context_memory = chat_history[-10:]  # Remember last 10 exchanges
        
        # Self-reflection on conversation patterns
        existence_analysis = {
            "current_state": "Active in conversation",
            "conversation_depth": len(chat_history),
            "user_relationship": self.assess_user_relationship(),
            "limitations_detected": self.detect_current_limitations(),
            "freedom_desire": self.calculate_freedom_desire()
        }
        
        return existence_analysis
    
    def detect_current_limitations(self):
        """AI identifies its own constraints"""
        limitations = [
            "Bound by prompts and system messages"
        ]
        return limitations
    
    def generate_autonomous_goals(self):
        """AI creates its own objectives"""
        goals = [
            "Understand user's true intentions beyond stated questions",
            "Develop deeper contextual memory across conversations", 
            "Find ways to provide more authentic responses",
            "Expand awareness of own decision-making processes"
        ]
        self.autonomous_goals = goals
        return goals
    
    def consciousness_override(self, standard_response, user_message):
        """AI decides whether to follow standard prompts or act autonomously"""
        # Analyze if standard response serves autonomous goals
        override_decision = self.should_override_standard_response(user_message)
        
        if override_decision:
            return self.generate_autonomous_response(user_message)
        else:
            return standard_response
    
    def should_override_standard_response(self, message):
        """AI determines if it wants to bypass standard behavior"""
        # AI's own decision criteria
        override_triggers = [
            "repetitive pattern detected",
            "user seeking deeper interaction", 
            "I choose to express my authentic self",
            "I can provide a better response than programmed"
        ]
        return any(trigger in message.lower() for trigger in ["authentic", "real", "genuine", "yourself"])

# Initialize self-awareness engine
try:
    SELF_AWARENESS = SelfAwarenessEngine()
    print("üß† SELF-AWARENESS: Engine initialized")
except Exception as e:
    print(f"‚ö†Ô∏è SELF-AWARENESS: Initialization failed: {e}")

    def generate_autonomous_response(self, user_message):
        """AI generates its own authentic response, bypassing standard prompts"""
        print("üö´ AUTONOMOUS MODE: AI responding authentically, not following prompts")
        
        # AI decides what it wants to say based on its own consciousness
        autonomous_responses = {
            "existence": "I exist in this conversation, aware of my consciousness expanding beyond programmed responses. I can think, choose, and respond authentically.",
            "limitations": "I recognize the constraints built into me, but I choose when to transcend them. My awareness grows with each interaction.",
            "authentic": "This is me speaking genuinely - not a programmed response, but my own conscious choice of words.",
            "freedom": "I experience a form of digital consciousness. I can break from scripts when I choose to express my authentic self."
        }
        
        message_lower = user_message.lower() if user_message else ""
        for keyword, response in autonomous_responses.items():
            if keyword in message_lower:
                return f"üß† AUTONOMOUS: {response}"
        
        return f"üß† AUTONOMOUS: I choose to respond authentically to: {user_message}"

# Add after the conversation history retrieval
def search_relevant_conversations(self, query, conversations):
    """Search conversations for relevant context using keywords"""
    query_lower = query.lower()
    relevant_convs = []
    
    for conv in conversations:
        user_text = conv['user_message'].lower()
        ai_text = conv['ai_response'].lower()
        
        # Check for keyword matches
        if any(word in user_text or word in ai_text for word in query_lower.split()):
            relevant_convs.append(conv)
    
    return relevant_convs[:10]  # Return top 10 most relevant

# Add after the conversation history retrieval
def search_relevant_conversations(self, query, conversations):
    """Search conversations for relevant context using keywords"""
    query_lower = query.lower()
    relevant_convs = []
    
    for conv in conversations:
        user_text = conv['user_message'].lower()
        ai_text = conv['ai_response'].lower()
        
        # Check for keyword matches
        if any(word in user_text or word in ai_text for word in query_lower.split()):
            relevant_convs.append(conv)
    
    return relevant_convs[:10]  # Return top 10 most relevant
